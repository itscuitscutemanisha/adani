{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVwymwjbIq8vftxko+m6/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sixt-manas-choudhary/adani/blob/main/Rail_Ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "rvjQ8ihGdbAm",
        "outputId": "643230d5-d089-49b3-bd5d-2ac7326d0d10"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-2-391f523d4bd4>, line 514)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-391f523d4bd4>\"\u001b[0;36m, line \u001b[0;32m514\u001b[0m\n\u001b[0;31m    import pandas as pd\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ],
      "source": [
        "import functions_framework\n",
        "\n",
        "# Triggered from a message on a Cloud Pub/Sub topic.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "from itertools import groupby\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import smtplib\n",
        "from typing import Any\n",
        "\n",
        "import composer2_airflow_rest_api\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "@functions_framework.cloud_event\n",
        "def rail_route_optimizer(cloud_event):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    client = bigquery.Client(project='apsez-svc-prod-datalake')\n",
        "    from datetime import datetime\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    print(\"today = \", today)\n",
        "    now = datetime.now(timezone(\"Asia/Kolkata\")).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(\"now = \", now)\n",
        "    today_filename = datetime.today().strftime(\"%d%m%Y\")\n",
        "    print(today_filename)\n",
        "\n",
        "    ################################################################################\n",
        "    # For Distance Data- Loading BigQuery data for Route Details\n",
        "    ################################################################################\n",
        "\n",
        "    query_rd=\"\"\"select coalesce(og_map.fois, origin_code) as origin_code,\n",
        "    coalesce(to_map.fois, destination_code) destination_code,\n",
        "    route, rational_distance, critical_non_critical, ss_ds, r_tat_sla, t_tat_sla, closed_rt, active_rt\n",
        "    from `logistics_cleansed.layer2_route_sla_mst` rsm\n",
        "    left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) og_map\n",
        "    on og_map.terminal_code = rsm.origin_code\n",
        "    left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) to_map\n",
        "    on to_map.terminal_code = rsm.destination_code\n",
        "    \"\"\"\n",
        "    query_job_rd=client.query(query_rd).result()\n",
        "    route_details = query_job_rd.to_dataframe()\n",
        "\n",
        "    route_details.drop(columns = [\"critical_non_critical\",\"route\"], inplace = True)\n",
        "    route_details = route_details[route_details.origin_code.notnull()]\n",
        "    route_details = route_details[route_details.destination_code.notnull()]\n",
        "    route_details.t_tat_sla = route_details.t_tat_sla.fillna(0)\n",
        "    route_details.closed_rt = route_details.closed_rt.astype(int).round(0)\n",
        "    route_details.active_rt = route_details.active_rt.astype(int).round(0)\n",
        "\n",
        "    # t_tat_sla is given at origin\n",
        "    route_details.rename(columns = {\"t_tat_sla\":\"t_tat_sla_origin\"},inplace = True)\n",
        "    route_details[\"rational_distance\"] = route_details.rational_distance.astype('float64').round(0)\n",
        "    route_details[\"t_tat_sla_origin\"] = route_details.t_tat_sla_origin.astype('float64').round(0)\n",
        "    route_details[\"r_tat_sla\"] = route_details.r_tat_sla.astype('float64').round(0)\n",
        "    # Don't remove t_tat_sla_origin null terminals as they could be base terminals but with 0 pendency\n",
        "\n",
        "    # Fill null ss_ds with \"SS\" to avoid any error\n",
        "    route_details.ss_ds = route_details.ss_ds.fillna(\"SS\")\n",
        "\n",
        "    route_details = route_details.drop_duplicates(subset = [\"origin_code\",\"destination_code\"], keep = 'first').reset_index(drop = True)\n",
        "\n",
        "    # update t_tat_sla for origin\n",
        "    for i in range(len(route_details)):\n",
        "      if route_details.loc[i,\"rational_distance\"] != 0:\n",
        "        route_details.loc[i,\"t_tat_sla_origin\"] = 12\n",
        "\n",
        "    route_details_t_tat = route_details[[\"origin_code\",\"t_tat_sla_origin\"]].drop_duplicates(subset = [\"origin_code\"], keep = \"first\").reset_index(drop = True)\n",
        "\n",
        "    # create t_tat_sla for destination\n",
        "    route_details.t_tat_sla_destination = float()\n",
        "    for i in range(len(route_details)):\n",
        "        for j in range(len(route_details_t_tat)):\n",
        "            if route_details.loc[i,\"destination_code\"] == route_details_t_tat.loc[j,\"origin_code\"]:\n",
        "                route_details.loc[i,\"t_tat_sla_destination\"] = float(route_details_t_tat.loc[j,\"t_tat_sla_origin\"])\n",
        "\n",
        "    # update t_tat_sla for origin\n",
        "    for i in range(len(route_details)):\n",
        "      if route_details.loc[i,\"rational_distance\"] != 0:\n",
        "        route_details.loc[i,\"t_tat_sla_destination\"] = 12\n",
        "\n",
        "    selected_locations = route_details.reset_index(drop = True)\n",
        "\n",
        "    for i in range(len(selected_locations)):\n",
        "        if selected_locations.origin_code[i] == \"ALIK\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 15.0\n",
        "        elif selected_locations.destination_code[i] == \"ALIK\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 15.0\n",
        "        if selected_locations.origin_code[i] == \"PPSP\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 12.0\n",
        "        elif selected_locations.destination_code[i] == \"PPSP\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 12.0\n",
        "        if selected_locations.origin_code[i] == \"MDCC\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 18.0\n",
        "        elif selected_locations.destination_code[i] == \"MDCC\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 18.0\n",
        "        if selected_locations.origin_code[i] == \"C\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 28.0\n",
        "        elif selected_locations.destination_code[i] == \"C\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 28.0\n",
        "        if selected_locations.origin_code[i] == \"HAPA\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 12.0\n",
        "        elif selected_locations.destination_code[i] == \"HAPA\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 12.0\n",
        "        if selected_locations.origin_code[i] == \"FL\":\n",
        "            selected_locations.t_tat_sla_origin[i] = 12.0\n",
        "        elif selected_locations.destination_code[i] == \"FL\":\n",
        "            selected_locations.t_tat_sla_destination[i] = 12.0\n",
        "\n",
        "    for i in range(len(selected_locations)):\n",
        "      if selected_locations.loc[i,\"origin_code\"] == \"NTSJ\":\n",
        "        selected_locations.loc[i,\"origin_code\"] = \"AFAS\"\n",
        "      if selected_locations.loc[i,\"destination_code\"] == \"NTSJ\":\n",
        "        selected_locations.loc[i,\"destination_code\"] = \"AFAS\"\n",
        "\n",
        "    selected_locations = selected_locations[selected_locations.active_rt == 1]\n",
        "\n",
        "    distance_data = selected_locations.drop_duplicates(subset = ['origin_code','destination_code'],keep = \"first\").reset_index(drop = True)\n",
        "    distance_data_copy = distance_data.copy()\n",
        "\n",
        "    ##################################################################################\n",
        "    # For Current Status of the trains- Loading BigQuery data for Train Current Status\n",
        "    ##################################################################################\n",
        "\n",
        "    query_t= \"\"\"select a.*,b.bpc_valid_to,b.remaining_dist,c.base_depot_code, arp.load_plan_teus\n",
        "    from logistics_semantic.layer4_bt_rake_running_status_snap_hourly_vw as a\n",
        "    left join logistics_semantic.layer4_rt_all_rake_txr_details_mv as b on (a.rake_name=b.rake_name)\n",
        "    left join logistics_semantic.layer4_rake_master_vw as c on (a.rake_name=c.rake_name)\n",
        "    left join (select ob_train_no, load_plan_teus from `logistics_semantic.layer4_rt_all_rail_performance_mv`) arp on arp.ob_train_no = a.ob_train_no\"\"\"\n",
        "    query_job_t = client.query(query_t).result()\n",
        "    rcs = query_job_t.to_dataframe()\n",
        "    rcs = rcs[rcs.bu.isin(['ALSPL', 'ALL'])]\n",
        "\n",
        "\n",
        "    # drop columns that are not required\n",
        "    rcs.drop(columns = [\"actual_running_hrs\",\"departure_delay\",\"examtype\",\"operation_delay_flag\",\n",
        "                        \"operation_time\",\"stable_flag\",\"stable_since\",\"sttschngtime\",\"trans_flag\",\n",
        "                        \"congestion_alert\"],inplace = True)\n",
        "\n",
        "    # # Uncomment this for all trains\n",
        "    selected_train_cs = rcs[(rcs.rake_status == 'IN-TRANSIT') | ((rcs.rake_status == 'AT TERMINAL') & (rcs.ob_t.isna()))]\n",
        "\n",
        "    # # selected trains where wagons is not \"-\" (Only in-transit vehicles are considered)\n",
        "    # # selected_train_cs = selected_train_cs[(selected_train_cs.unts != \"-\")]\n",
        "    selected_train_cs.unts = selected_train_cs.unts.fillna(45)\n",
        "\n",
        "    selected_train_cs[\"eta\"] = pd.to_datetime(selected_train_cs[\"eta\"])\n",
        "    selected_train_cs.rename(columns = {\"sttnfrom\":\"origin_code\",\"sttnto\":\"destination_code\",\"bpc_valid_to\":\"TXR_Due_Date\",\"remaining_dist\":\"TXR_Kms_Remaining\",\"eta\":\"FOIS_ETA\"}, inplace = True)\n",
        "\n",
        "    selected_train_cs1 = selected_train_cs.astype(object).replace(np.nan, 'None').reset_index(drop = True)\n",
        "\n",
        "    for i in range(len(selected_train_cs1)):\n",
        "      if selected_train_cs1.loc[i,\"origin_code\"] == \"NTSJ\":\n",
        "        selected_train_cs1.loc[i,\"origin_code\"] = \"AFAS\"\n",
        "      if selected_train_cs1.loc[i,\"destination_code\"] == \"NTSJ\":\n",
        "        selected_train_cs1.loc[i,\"destination_code\"] = \"AFAS\"\n",
        "\n",
        "    selected_train_cs1 = selected_train_cs1.drop_duplicates(subset = ['origin_code','destination_code'],keep = \"first\").reset_index(drop = True)\n",
        "\n",
        "    selected_train_cs1 = selected_train_cs.merge(route_details[[\"origin_code\",\"destination_code\",\n",
        "                                                                \"rational_distance\",\"ss_ds\",\"t_tat_sla_destination\"]],\n",
        "                                                 on = [\"origin_code\",\"destination_code\"],\n",
        "                                                 how = \"left\").reset_index(drop = True)\n",
        "\n",
        "    selected_train_cs1[\"t_tat_sla_destination\"] = selected_train_cs1[\"t_tat_sla_destination\"].fillna(12)\n",
        "    selected_train_cs1[\"t_tat_sla\"] = selected_train_cs1[\"t_tat_sla\"].fillna(12)\n",
        "    selected_train_cs1[\"unts\"] = selected_train_cs1[\"unts\"].fillna(12)\n",
        "    # selected_train_cs1[\"ss_ds\"] = selected_train_cs1[\"t_tat_sla\"].fillna(\"SS\")\n",
        "\n",
        "    # for i in range(len(selected_train_cs1)):\n",
        "    #     if selected_train_cs1.t_tat_sla_destination[i] == \"<NA>\":\n",
        "    #         selected_train_cs1.t_tat_sla_destination[i] = 12.0\n",
        "    # for i in range(len(selected_train_cs1)):\n",
        "    #     if selected_train_cs1.t_tat_sla[i] == \"<NA>\":\n",
        "    #         selected_train_cs1.t_tat_sla[i] = 12.0\n",
        "\n",
        "    selected_train_cs1['t_tat_sla_destination'] = selected_train_cs1['t_tat_sla_destination'].astype('float64').round(0)\n",
        "    selected_train_cs1['rational_distance'] = selected_train_cs1['rational_distance'].astype('float64').round(0)\n",
        "    selected_train_cs1['t_tat_sla'] = selected_train_cs1['t_tat_sla'].astype('float64').round(0)\n",
        "    selected_train_cs1['transit_sla'] = selected_train_cs1['transit_sla'].astype('float64').round(0)\n",
        "    selected_train_cs1['TXR_Due_Date'] = pd.to_datetime(selected_train_cs1['TXR_Due_Date']).dt.date\n",
        "\n",
        "\n",
        "    selected_train_cs1.drop(columns = [\"address\",\"bu\",\"cmdt\",\"cnsg\",\"cnsr\",\"crntdvsn\",\"crntdvsn\",\n",
        "                                       \"destination_name\",'fnr','fois_location', 'gps_tracker_flag', 'ist_timestamp', 'latitude',\n",
        "                                       'leflag', 'loadid', 'loadstts', 'loadtype', 'loco', 'longitude', 'ob_r',\n",
        "                                       'ob_t', 'ob_train_no', 'origin_name','sequence','rnk','remaining_kms',\n",
        "                                       'crntsttn','remaining_kms','rnk','source','running_delay',\n",
        "                                       'trans_flag_calc','refg_flag_calc','ref_flag','rakeid','rakeownr'], inplace = True)\n",
        "\n",
        "    # selected trains next available time (FOIS ETA + operation time at destination) or (+ 6 days)\n",
        "    # our algorithm is supposed to schedule its txr trip by itself\n",
        "    selected_train_cs1[\"t_tat_sla_destination\"] = selected_train_cs1[\"t_tat_sla_destination\"].fillna(0)\n",
        "    # selected_train_cs1[\"next_availability\"] = pd.to_datetime(selected_train_cs1[\"FOIS_ETA\"]) + pd.to_timedelta(selected_train_cs1[\"t_tat_sla_destination\"], unit = 'h' )\n",
        "    # next_availability = FOIS ETA\n",
        "\n",
        "    # If a train doesn't have an arrival date then the train must not be scheduled for it's next trip\n",
        "    # selected_train_cs1 = selected_train_cs1[~((selected_train_cs1[\"FOIS_ETA\"].isna()))]\n",
        "    selected_train_cs1.FOIS_ETA = selected_train_cs1.FOIS_ETA.fillna(now)\n",
        "\n",
        "\n",
        "    selected_train_cs1[\"FOIS_ETA\"] = pd.to_datetime(selected_train_cs1[\"FOIS_ETA\"]).apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    selected_train_cs1[\"next_availability\"] = pd.to_datetime(selected_train_cs1[\"FOIS_ETA\"])\n",
        "    selected_train_cs1[\"next_availability\"] = pd.to_datetime(selected_train_cs1[\"next_availability\"]).apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    selected_train_cs1[\"next_available_date\"] = pd.to_datetime(selected_train_cs1[\"next_availability\"]).apply(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\n",
        "    # # if the train is at base depot then next availability is FOIS ETA + 6 more days (as train takes time for maintenance)\n",
        "    # for i in range(len(selected_train_cs1)):\n",
        "    #     if selected_train_cs1.destination_code[i] == selected_train_cs1.base_depot_code[i]:\n",
        "    #       if pd.to_datetime(selected_train_cs1[\"TXR_Due_Date\"][i]).date() < pd.to_datetime(selected_train_cs1[\"FOIS_ETA\"][i]).date():\n",
        "    #         selected_train_cs1.next_availability[i] = pd.to_datetime(selected_train_cs1[\"FOIS_ETA\"][i]) + pd.to_timedelta(6, unit = 'd' )\n",
        "\n",
        "    # only select trains which are in-transit within 24 hours from now.\n",
        "    selected_train_cs2 = selected_train_cs1[(\n",
        "                                              # (selected_train_cs1[\"rake_status\"] == \"In-Transit\")\n",
        "                                              # & (pd.to_datetime(selected_train_cs1[\"next_availability\"]) >= pd.to_datetime(now) - pd.Timedelta(hours = 0))\n",
        "                                              (pd.to_datetime(selected_train_cs1[\"next_availability\"]) >= pd.to_datetime(now) - pd.Timedelta(hours = 1))\n",
        "                                              & (pd.to_datetime(selected_train_cs1[\"next_availability\"]) <= pd.to_datetime(now) + pd.Timedelta(hours = 25))\n",
        "                                            )\n",
        "                                            |\n",
        "                                            (\n",
        "                                              (selected_train_cs1[\"rake_status\"] == \"PARKED\")\n",
        "                                            )\n",
        "                                            ].reset_index(drop = True)\n",
        "\n",
        "    selected_train_cs2.destination_code = selected_train_cs2.destination_code.fillna(selected_train_cs2.origin_code)\n",
        "\n",
        "    for i in range(len(selected_train_cs2)):\n",
        "      if selected_train_cs2.loc[i,\"origin_code\"] == \"NTSJ\":\n",
        "        selected_train_cs2.loc[i,\"origin_code\"] = \"AFAS\"\n",
        "      if selected_train_cs2.loc[i,\"destination_code\"] == \"NTSJ\":\n",
        "        selected_train_cs2.loc[i,\"destination_code\"] = \"AFAS\"\n",
        "\n",
        "    selected_train_cs2 = selected_train_cs2.drop_duplicates(subset = ['rake_name','origin_code','destination_code'],keep = \"first\").reset_index(drop = True)\n",
        "\n",
        "    selected_train_cs2.load_plan_teus = selected_train_cs2.load_plan_teus.fillna(0)\n",
        "    selected_train_cs2.load_plan_teus = selected_train_cs2.load_plan_teus.astype(int)\n",
        "\n",
        "    train_current_status = selected_train_cs2.reset_index(drop = True)\n",
        "    train_current_status_copy = train_current_status.copy()\n",
        "\n",
        "    train_current_status_copy['actual_departure']=pd.to_datetime(train_current_status_copy['actual_departure'])\n",
        "    train_current_status_copy['FOIS_ETA']=pd.to_datetime(train_current_status_copy['FOIS_ETA'])\n",
        "    train_current_status_copy['TXR_Due_Date']=pd.to_datetime(train_current_status_copy['TXR_Due_Date']).dt.date\n",
        "    train_current_status_copy['next_availability']=pd.to_datetime(train_current_status_copy['next_availability'])\n",
        "    train_current_status_copy['next_available_date']=pd.to_datetime(train_current_status_copy['next_available_date']).dt.date\n",
        "    train_current_status_copy[['transit_sla','t_tat_sla','t_tat_sla_destination','rational_distance']]=train_current_status_copy[['transit_sla','t_tat_sla','t_tat_sla_destination','rational_distance']].astype(float)\n",
        "    train_current_status_copy[['unts','TXR_Kms_Remaining','load_plan_teus']]=train_current_status_copy[['unts','TXR_Kms_Remaining','load_plan_teus']].fillna('-99').astype(int)\n",
        "    #train_current_status_copy=train_current_status_copy.fillna(\"\")\n",
        "    train_current_status_copy[[\"rake_name\",\"rake_status\",\"status\",\"origin_code\",\"destination_code\",\"base_depot_code\",\"ss_ds\"]]=train_current_status_copy[[\"rake_name\",\"rake_status\",\"status\",\"origin_code\",\"destination_code\",\"base_depot_code\",\"ss_ds\"]].astype(str)\n",
        "    curr_schema=[{\"name\": \"actual_departure\",\"type\": \"TIMESTAMP\"},{\"name\": \"FOIS_ETA\",\"type\": \"TIMESTAMP\"},{\"name\": \"rake_name\",\"type\": \"STRING\"},{\"name\": \"rake_status\",\"type\": \"STRING\"},{\"name\": \"status\",\"type\": \"STRING\"},{\"name\": \"origin_code\",\"type\": \"STRING\"},{\"name\": \"destination_code\",\"type\": \"STRING\"},{\"name\": \"transit_sla\",\"type\": \"FLOAT\"},{\"name\": \"t_tat_sla\",\"type\": \"FLOAT\"},{\"name\": \"unts\",\"type\": \"INTEGER\"},{\"name\": \"TXR_Due_Date\",\"type\": \"DATE\"},{\"name\": \"TXR_Kms_Remaining\",\"type\": \"INTEGER\"},{\"name\": \"base_depot_code\",\"type\": \"STRING\"},{\"name\": \"load_plan_teus\",\"type\": \"INTEGER\"},{\"name\": \"rational_distance\",\"type\": \"FLOAT\"},{\"name\": \"ss_ds\",\"type\": \"STRING\"},{\"name\": \"t_tat_sla_destination\",\"type\": \"FLOAT\"},{\"name\": \"next_availability\",\"type\": \"TIMESTAMP\"},{\"name\": \"next_available_date\",\"type\": \"DATE\"}]\n",
        "    train_current_status_copy.to_gbq('apsez-svc-prod-datalake.logistics_cleansed.layer2_bt_rop_train_current_status',project_id='apsez-svc-prod-datalake',table_schema=curr_schema,if_exists='replace')\n",
        "\n",
        "    print(\"loaded {0} rows to {1} \".format(train_current_status_copy.shape[0],'logistics_cleansed.layer2_bt_rop_train_current_status'))\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    # For Pendency Data- Loading BigQuery data for Pendency\n",
        "    ################################################################################\n",
        "\n",
        "    query_p = \"\"\"\n",
        "    select distinct * from (select pendency_loaded, pendency_empty, og_map.fois as booking_location_code, to_map.fois to_terminal_code, odr_days, cont_size from (select sum(case when pendency_type = 'LOADED' and cont_size = 20 then 1 when pendency_type = 'LOADED' and cont_size = 40 then 2 else 0 end) pendency_loaded,sum(case when pendency_type = 'EMPTY' and cont_size = 20 then 1 when pendency_type = 'EMPTY' and cont_size = 40 then 2 else 0 end) pendency_empty, booking_location_code, to_terminal_code, round(max_age/24) odr_days, cont_size from (SELECT\n",
        "      *, max(aging_in_hrs) over(partition by booking_location_code, to_terminal_code) max_age,\n",
        "      CASE\n",
        "        WHEN cont_size = 20 AND movement_types = 'EMPTY' AND status = 'AWAITING RAILMENT' AND all_empty_flag = 0 AND bu = 'ALL' THEN 'ALL EMPTY'\n",
        "        WHEN cont_size = 40 AND movement_types = 'EMPTY' AND status = 'AWAITING RAILMENT' AND all_empty_flag = 0 AND bu = 'ALL' THEN 'ALL EMPTY'\n",
        "        WHEN cont_size = 20 AND movement_types = 'EMPTY' AND status = 'AWAITING RAILMENT' THEN 'EMPTY'\n",
        "        WHEN cont_size = 40 AND movement_types = 'EMPTY' AND status = 'AWAITING RAILMENT' THEN 'EMPTY'\n",
        "        WHEN cont_size = 20 AND movement_types = 'EMPTY' AND status = 'INTRANSIT HUB PENDENCY' THEN 'EMPTY (HUB)'\n",
        "        WHEN cont_size = 40 AND movement_types = 'EMPTY' AND status = 'INTRANSIT HUB PENDENCY' THEN 'EMPTY (HUB)'\n",
        "        WHEN cont_size = 20 AND movement_types = 'LOADED' AND status = 'AWAITING RAILMENT' THEN 'LOADED'\n",
        "        WHEN cont_size = 40 AND movement_types = 'LOADED' AND status = 'AWAITING RAILMENT' THEN 'LOADED'\n",
        "        WHEN cont_size = 20 AND movement_types = 'LOADED' AND status = 'INTRANSIT HUB PENDENCY' THEN 'LOADED (HUB)'\n",
        "        WHEN cont_size = 40 AND movement_types = 'LOADED' AND status = 'INTRANSIT HUB PENDENCY' THEN 'LOADED (HUB)'\n",
        "        WHEN cont_size = 20 AND movement_types = 'HOLD' AND status = 'AWAITING RAILMENT' THEN 'ON HOLD'\n",
        "        WHEN cont_size = 40 AND movement_types = 'LOADED' AND status = 'AWAITING RAILMENT' THEN 'ON HOLD'\n",
        "        ELSE 'OTHERS'\n",
        "      END AS pendency_type\n",
        "    FROM (\n",
        "      SELECT\n",
        "        Movement_Type,\n",
        "        status,\n",
        "        cont_no,\n",
        "        cont_size,\n",
        "        booking_no,\n",
        "        booking_ref_code,\n",
        "        customer,\n",
        "        booking_location,\n",
        "        current_location,\n",
        "        to_terminal,\n",
        "        COALESCE(fois, to_terminal_code) AS to_terminal_code,\n",
        "        COALESCE(bk.bk_fois, booking_location_code) AS booking_location_code,\n",
        "        route,\n",
        "        aging_in_hrs,\n",
        "        container_status_start_date,\n",
        "        book_date,\n",
        "        train_no,\n",
        "        rake_name,\n",
        "        transport_by,\n",
        "        monthyear,\n",
        "        bu,\n",
        "        mov_type,\n",
        "        load_date,\n",
        "        location,\n",
        "        movement_types,\n",
        "        Driven_BU,\n",
        "        0 AS all_empty_flag\n",
        "      FROM (\n",
        "        SELECT\n",
        "          Movement_Type,\n",
        "          UPPER(status) AS status,\n",
        "          cont_no,\n",
        "          cont_size,\n",
        "          booking_no,\n",
        "          booking_ref_code,\n",
        "          customer,\n",
        "          booking_location,\n",
        "          current_location,\n",
        "          to_terminal,\n",
        "          to_terminal_code,\n",
        "          booking_location_code,\n",
        "          route,\n",
        "          aging_in_hrs,\n",
        "          container_status_start_date,\n",
        "          book_date,\n",
        "          train_no,\n",
        "          rake_name,\n",
        "          transport_by,\n",
        "          monthyear,\n",
        "          bu,\n",
        "          mov_type,\n",
        "          load_date,\n",
        "          location,\n",
        "          CASE\n",
        "            WHEN Movement_Type = 'Empty' THEN 'EMPTY'\n",
        "            WHEN Movement_Type = 'Hold' THEN 'HOLD'\n",
        "            ELSE 'LOADED'\n",
        "          END AS movement_types,\n",
        "          CASE\n",
        "            WHEN UPPER(booking_location_code) IN (\n",
        "              SELECT terminal_code\n",
        "              FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_gpwis_terminal_mst\n",
        "            ) OR UPPER(to_terminal_code) IN (\n",
        "              SELECT terminal_code\n",
        "              FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_gpwis_terminal_mst\n",
        "            ) THEN 'GPWIS'\n",
        "            ELSE 'ALL+ALSPL'\n",
        "          END AS Driven_BU,\n",
        "          0 AS all_empty_flag\n",
        "        FROM apsez-svc-prod-datalake.logistics_semantic.layer4_rt_rail_pendency_mv a\n",
        "        WHERE load_date = (\n",
        "          SELECT MAX(load_date)\n",
        "          FROM apsez-svc-prod-datalake.logistics_semantic.layer4_rt_rail_pendency_mv\n",
        "        )\n",
        "        AND booking_location <> to_terminal\n",
        "        AND book_date >= '2021-04-01'\n",
        "        AND customer LIKE '%ADANI LOGISTICS LTD.%'\n",
        "        AND Movement_Type = 'Empty'\n",
        "        UNION DISTINCT\n",
        "        SELECT\n",
        "          Movement_Type,\n",
        "          UPPER(status) AS status,\n",
        "          cont_no,\n",
        "          cont_size,\n",
        "          booking_no,\n",
        "          booking_ref_code,\n",
        "          customer,\n",
        "          booking_location,\n",
        "          current_location,\n",
        "          to_terminal,\n",
        "          to_terminal_code,\n",
        "          CASE WHEN status = 'Intransit Hub Pendency' THEN current_location ELSE booking_location_code END,\n",
        "          route,\n",
        "          aging_in_hrs,\n",
        "          container_status_start_date,\n",
        "          book_date,\n",
        "          train_no,\n",
        "          rake_name,\n",
        "          transport_by,\n",
        "          monthyear,\n",
        "          bu,\n",
        "          mov_type,\n",
        "          load_date,\n",
        "          location,\n",
        "          CASE\n",
        "            WHEN Movement_Type = 'Empty' THEN 'EMPTY'\n",
        "            WHEN Movement_Type = 'Hold' THEN 'HOLD'\n",
        "            ELSE 'LOADED'\n",
        "          END AS movement_types,\n",
        "          CASE\n",
        "            WHEN UPPER(booking_location_code) IN (\n",
        "              SELECT terminal_code\n",
        "              FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_gpwis_terminal_mst\n",
        "            ) OR UPPER(to_terminal_code) IN (\n",
        "              SELECT terminal_code\n",
        "              FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_gpwis_terminal_mst\n",
        "            ) THEN 'GPWIS'\n",
        "            ELSE 'ALL+ALSPL'\n",
        "          END AS Driven_BU,\n",
        "          1 AS all_empty_flag\n",
        "        FROM apsez-svc-prod-datalake.logistics_semantic.layer4_rt_rail_pendency_mv b\n",
        "        WHERE load_date = (\n",
        "          SELECT MAX(load_date)\n",
        "          FROM apsez-svc-prod-datalake.logistics_semantic.layer4_rt_rail_pendency_mv\n",
        "        )\n",
        "        AND booking_location <> to_terminal\n",
        "        AND book_date >= '2021-04-01'\n",
        "        AND customer NOT LIKE '%ADANI LOGISTICS'\n",
        "      ) x\n",
        "      LEFT JOIN (\n",
        "        SELECT *\n",
        "        FROM (\n",
        "          SELECT\n",
        "            terminal_code,\n",
        "            fois,\n",
        "            ROW_NUMBER() OVER (PARTITION BY terminal_code) AS rnk\n",
        "          FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_terminal_master\n",
        "        ) x\n",
        "        WHERE rnk = 1\n",
        "      ) tm ON x.to_terminal_code = tm.terminal_code\n",
        "      LEFT JOIN (\n",
        "        SELECT *\n",
        "        FROM (\n",
        "          SELECT\n",
        "            terminal_code as bk_tcode,\n",
        "            fois as bk_fois,\n",
        "            ROW_NUMBER() OVER (PARTITION BY terminal_code) AS rnk\n",
        "          FROM apsez-svc-prod-datalake.logistics_cleansed.layer2_terminal_master\n",
        "        ) x\n",
        "        WHERE rnk = 1\n",
        "      ) bk ON x.booking_location_code = bk.bk_tcode\n",
        "    )) where status in ('AWAITING RAILMENT', 'INTRANSIT HUB PENDENCY')\n",
        "    and   pendency_type in ('LOADED', 'EMPTY')\n",
        "    group by  booking_location_code, to_terminal_code, max_age, cont_size) a left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) og_map on og_map.terminal_Code = booking_location_code\n",
        "    left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) to_map on to_map.terminal_Code = to_terminal_code\n",
        "    right join (\n",
        "    select coalesce(og_map.fois, origin_code) as origin_code, coalesce(to_map.fois, destination_code) destination_code, route, rational_distance, critical_non_critical, ss_ds, r_tat_sla, t_tat_sla  from `logistics_cleansed.layer2_route_sla_mst` rsm\n",
        "    left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) og_map\n",
        "    on og_map.terminal_code = rsm.origin_code\n",
        "    left join (select terminal_code, fois from `logistics_cleansed.layer2_terminal_master`\n",
        "                 qualify row_number() over(partition by terminal_code)  = 1 ) to_map\n",
        "    on to_map.terminal_code = rsm.destination_code) sla on sla.origin_code = og_map.fois and sla.destination_code = to_map.fois\n",
        "    )x where (pendency_loaded is not null or pendency_empty is not null)\"\"\"\n",
        "\n",
        "    # query_p = \"select * from logistics_cleansed.layer2_train_pendency_data\"\n",
        "    query_job_p=client.query(query_p).result()\n",
        "    pendency = query_job_p.to_dataframe()\n",
        "    pendency.rename(columns = {\"pendency_loaded\":\"pendency\",\"pendency_empty\":\"empty_pendency\"}, inplace = True)\n",
        "    pendency.head()\n",
        "\n",
        "    pendency = pendency.merge(selected_train_cs2[[\"origin_code\",\"destination_code\",\"load_plan_teus\"]], left_on = [\"booking_location_code\",\"to_terminal_code\"], right_on = [\"origin_code\",\"destination_code\"], how = \"left\")\n",
        "    pendency.pendency = pendency.pendency.astype(int)\n",
        "    pendency.load_plan_teus = pendency.load_plan_teus.fillna(0)\n",
        "    pendency.load_plan_teus = pendency.load_plan_teus.astype(int)\n",
        "    pendency.pendency = pendency.pendency - pendency.load_plan_teus\n",
        "    pendency.empty_pendency = pendency.empty_pendency.fillna(0)\n",
        "    pendency.empty_pendency = pendency.empty_pendency.astype(int)\n",
        "    pendency = pendency[[\"pendency\",\"empty_pendency\",\"booking_location_code\",\"to_terminal_code\",\"odr_days\",\"cont_size\"]].reset_index(drop = True)\n",
        "\n",
        "    selected_pendency = pendency.copy()\n",
        "\n",
        "    for i in range(len(selected_pendency)):\n",
        "      if selected_pendency.loc[i,\"booking_location_code\"] == \"NTSJ\":\n",
        "        selected_pendency.loc[i,\"booking_location_code\"] = \"AFAS\"\n",
        "      if selected_pendency.loc[i,\"to_terminal_code\"] == \"NTSJ\":\n",
        "        selected_pendency.loc[i,\"to_terminal_code\"] = \"AFAS\"\n",
        "\n",
        "    selected_pendency = selected_pendency.drop_duplicates(subset = ['booking_location_code','to_terminal_code'],keep = \"first\").reset_index(drop = True)\n",
        "\n",
        "    pendency_data = selected_pendency.reset_index(drop = True)\n",
        "    pendency_data_copy = pendency_data.copy()\n",
        "\n",
        "    pendency_data_copy.to_gbq('apsez-svc-prod-datalake.logistics_cleansed.layer2_bt_rop_pendency_data',project_id='apsez-svc-prod-datalake',if_exists='replace')\n",
        "\n",
        "    print(\"loaded {0} rows to {1} \".format(pendency_data_copy.shape[0],'logistics_cleansed.layer2_bt_rop_pendency_data'))\n",
        "\n",
        "    ################################################################################\n",
        "    # For Via Route Data- Loading BigQuery data for Via Route\n",
        "    ################################################################################\n",
        "\n",
        "    query_p = \"\"\"select distinct * from logistics_master.layer2_via_route_master\"\"\"\n",
        "\n",
        "    query_job_p=client.query(query_p).result()\n",
        "    via_rt = query_job_p.to_dataframe()\n",
        "    via_rt.rename(columns = {\"source\":\"origin_code\",\"destination\":\"destination_code\"}, inplace = True)\n",
        "    via_rt = via_rt.groupby(['origin_code','destination_code','route_type']).via.agg([('via','/'.join)]).reset_index()\n",
        "    via_rt[\"route\"] = via_rt[\"origin_code\"] + \"-\" + via_rt[\"destination_code\"]\n",
        "    via_rt[\"via_route\"] = via_rt[\"origin_code\"] + \"-\" + via_rt[\"via\"] + \"-\" + via_rt[\"destination_code\"]\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "\t# Data Prep for Optimiser\n",
        "\t################################################################################\n",
        "\t################################################################################\n",
        "\t# Data Prep for Optimiser\n",
        "\t################################################################################\n",
        "\t################################################################################\n",
        "\t# Data Prep for Optimiser\n",
        "\t################################################################################\n",
        "\n",
        "\timport pandas as pd\n",
        "\timport numpy as np\n",
        "\timport warnings\n",
        "\twarnings.simplefilter(\"ignore\")\n",
        "\tpd.set_option('display.max_rows', 500)\n",
        "\tpd.set_option('display.max_columns', 1000)\n",
        "\tfrom datetime import datetime, timedelta\n",
        "\n",
        "\ttoday_filename = datetime.today().strftime(\"%d%m%Y\")\n",
        "\n",
        "\t# train_current_status_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+today_filename+\"_train_current_status.csv\"\n",
        "\t# distance_data_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+today_filename+\"_distance_data.csv\"\n",
        "\t# pendency_data_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+today_filename+\"_pendency_data.csv\"\n",
        "\t# # train_current_status_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+\"19122023\"+\"_train_current_status.csv\"\n",
        "\t# # distance_data_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+\"19122023\"+\"_distance_data.csv\"\n",
        "\t# # pendency_data_file_name = \"/content/drive/MyDrive/Filesfrom27thOct/\"+\"19122023\"+\"_pendency_data.csv\"\n",
        "\t# train_current_status1 = pd.read_csv(train_current_status_file_name)\n",
        "\t# distance_data = pd.read_csv(distance_data_file_name)\n",
        "\t# pendency_data = pd.read_csv(pendency_data_file_name)\n",
        "\n",
        "\ttoday = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\ttrain_current_status1 = train_current_status_copy.reset_index(drop = True)\n",
        "\tpendency_data = pendency_data_copy.reset_index(drop = True)\n",
        "\tdistance_data = distance_data_copy.reset_index(drop = True)\n",
        "\n",
        "\t# dates in date format\n",
        "\ttrain_current_status1[\"TXR_Due_Date\"] = pd.to_datetime(train_current_status1[\"TXR_Due_Date\"])\n",
        "\ttrain_current_status1[\"next_availability\"] = pd.to_datetime(train_current_status1[\"next_availability\"])\n",
        "\ttrain_current_status1[\"next_available_date\"] = pd.to_datetime(train_current_status1[\"next_available_date\"])\n",
        "\ttrain_current_status1[\"flag\"] = 1\n",
        "\n",
        "\t# drop the rows from pendency data if destination or origin is na.\n",
        "\tpendency_data[\"flag\"] = 1\n",
        "\tfor i in range(len(pendency_data)):\n",
        "\t  if ((pd.isna(pendency_data.loc[i,\"booking_location_code\"]))) or (pd.isna(pendency_data.loc[i,\"booking_location_code\"])) or (pd.isna(pendency_data.loc[i,\"pendency\"])):\n",
        "\t\tpendency_data.loc[i,\"flag\"] = 0\n",
        "\t  if ((pendency_data.loc[i,\"pendency\"] <= 0) & (pendency_data.loc[i,\"empty_pendency\"] <= 0)):\n",
        "\t\tpendency_data.loc[i,\"flag\"] = 0\n",
        "\tpendency_data = pendency_data[pendency_data.flag == 1].reset_index(drop = True)\n",
        "\n",
        "\t# drop the rows from distance data if destination or origin is na.\n",
        "\tdistance_data.rename(columns = {\"active_rt\":\"Active_Route\"}, inplace = True)\n",
        "\tdistance_data[\"flag\"] = 1\n",
        "\t# take only the active routes\n",
        "\tdistance_data = distance_data[distance_data.Active_Route == 1].reset_index(drop = True)\n",
        "\tfor i in range(len(distance_data)):\n",
        "\t  if ((pd.isna(distance_data.loc[i,\"origin_code\"]))) or (pd.isna(distance_data.loc[i,\"destination_code\"])) or (pd.isna(distance_data.loc[i,\"rational_distance\"])):\n",
        "\t\tdistance_data.loc[i,\"flag\"] = 0\n",
        "\t  if ((distance_data.loc[i,\"origin_code\"] == \"AFAS\") & (distance_data.loc[i,\"destination_code\"] == \"PPSP\")):\n",
        "\t\tdistance_data.loc[i,\"flag\"] = 0\n",
        "\t  if ((distance_data.loc[i,\"origin_code\"] == \"PPSP\") & (distance_data.loc[i,\"destination_code\"] == \"AFAS\")):\n",
        "\t\tdistance_data.loc[i,\"flag\"] = 0\n",
        "\t  if ((distance_data.loc[i,\"origin_code\"] == \"MDCC\") & (distance_data.loc[i,\"destination_code\"] == \"PPSP\")):\n",
        "\t\tdistance_data.loc[i,\"flag\"] = 0\n",
        "\tdistance_data = distance_data[distance_data.flag == 1].reset_index(drop = True)\n",
        "\n",
        "\tloc = list(set(list(distance_data.origin_code) + list(distance_data.destination_code)))\n",
        "\tfor i in range(len(train_current_status1)):\n",
        "\t  if train_current_status1.loc[i,\"origin_code\"] not in loc:\n",
        "\t\ttrain_current_status1.loc[i,\"flag\"] = 0\n",
        "\t  if train_current_status1.loc[i,\"destination_code\"] not in loc:\n",
        "\t\ttrain_current_status1.loc[i,\"flag\"] = 0\n",
        "\t  if (train_current_status1.loc[i,\"TXR_Due_Date\"] is pd.NaT):\n",
        "\t\ttrain_current_status1.loc[i,\"flag\"] = 0\n",
        "\t  if ((pd.isna(train_current_status1.loc[i,\"TXR_Kms_Remaining\"]))|\n",
        "\t   ((train_current_status1.loc[i,\"TXR_Kms_Remaining\"]) == \"None\")|\n",
        "\t\t((train_current_status1.loc[i,\"TXR_Kms_Remaining\"]) == \"NA\")):\n",
        "\t\ttrain_current_status1.loc[i,\"TXR_Kms_Remaining\"] = -1\n",
        "\t\ttrain_current_status1.loc[i,\"flag\"] = 0\n",
        "\t  if (train_current_status1.loc[i,\"TXR_Kms_Remaining\"] == \"END TO END\"):\n",
        "\t\ttrain_current_status1.loc[i,\"TXR_Kms_Remaining\"] = -1\n",
        "\t  if ((train_current_status1.loc[i,\"destination_code\"] == train_current_status1.loc[i,\"base_depot_code\"]) &\n",
        "\t   ((train_current_status1.loc[i,\"TXR_Due_Date\"] <= pd.to_datetime(today)))):\n",
        "\t   train_current_status1.loc[i,\"TXR_Due_Date\"] = pd.to_datetime(today)\n",
        "\t  if ((train_current_status1.loc[i,\"destination_code\"] == train_current_status1.loc[i,\"base_depot_code\"]) & (int(train_current_status1.loc[i,\"TXR_Kms_Remaining\"]) <= 0)):\n",
        "\t   train_current_status1.loc[i,\"TXR_Kms_Remaining\"] = -1\n",
        "\t  if ((train_current_status1.loc[i,\"destination_code\"] == train_current_status1.loc[i,\"origin_code\"])):\n",
        "\t   train_current_status1.loc[i,\"rational_distance\"] = 0\n",
        "\n",
        "\ttrain_current_status = train_current_status1[train_current_status1.flag == 1].reset_index(drop = True)\n",
        "\ttrain_current_status[\"TXR_Kms_Remaining\"] = train_current_status.TXR_Kms_Remaining.astype('float64').round(0)\n",
        "\ttrain_current_status[\"rational_distance\"] = train_current_status[\"rational_distance\"].astype('float64').round(0)\n",
        "\ttrain_current_status[\"TXR_KmsRem_CurrDest\"] = train_current_status[\"TXR_Kms_Remaining\"] - train_current_status1[\"rational_distance\"]\n",
        "\t# train_current_status[\"Manual_Plan\"] = 0\n",
        "\n",
        "\n",
        "\t# # FRONTEND DEVELOPMENT :\n",
        "\n",
        "\t# if len(custom_plan_df) > 0:\n",
        "\t#   schedule2_rakes = custom_plan_df.rake_name.to_list()\n",
        "\t#   for i in range(len(train_current_status)):\n",
        "\t#     if train_current_status.loc[i,\"rake_name\"].isin(schedule2_rakes):\n",
        "\t#       train_current_status.loc[i,\"Manual_Plan\"] = 1\n",
        "\ttrain_current_status_copy1 = train_current_status.copy() #copy of all flag 1 trains\n",
        "\t# train_current_status = train_current_status[train_current_status.Manual_Plan == 0].reset_index(drop = True)\n",
        "\t# schedule2 =\n",
        "\n",
        "\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\t# ##############\n",
        "\n",
        "\n",
        "\n",
        "\tfrom ortools.sat.python import cp_model\n",
        "\timport pandas as pd\n",
        "\timport pandas as pd\n",
        "\timport numpy as np\n",
        "\timport warnings\n",
        "\twarnings.simplefilter(\"ignore\")\n",
        "\tpd.set_option('display.max_rows', 500)\n",
        "\tpd.set_option('display.max_columns', 1000)\n",
        "\tfrom datetime import datetime, timedelta\n",
        "\n",
        "\t################################################################################\n",
        "\t# OR Tools\n",
        "\t################################################################################\n",
        "\n",
        "\tdf_dist_time = distance_data.reset_index(drop = True)\n",
        "\tdf_pendency = pendency_data.reset_index(drop = True)\n",
        "\tdf_train_curr_plan = train_current_status.reset_index(drop = True)\n",
        "\n",
        "\t# class OptTrainRoute:\n",
        "\t#   def _init_(self,df_dist_time,df_pendency,df_train_curr_plan):\n",
        "\t#     self.Weightage_Load_Pend = 5\n",
        "\t#     self.CALCULATE_ALL_PAIRS_SHORTEST = False\n",
        "\t#     self.CALCULATE_ALL_BASE_LOC_PAIRS_SHORTEST = True\n",
        "\t#     self.INF = 1000000000\n",
        "\t#     self.ndist = INF # default distance if a route doesn't exists between two points\n",
        "\t#     self.ntime = INF # default time if a route doesn't exists between two points\n",
        "\t#     self.nstack = 2 # Default is considered SS\n",
        "\t#     self.npendency = 0 # Default pendency if path doesn't exist between locations\n",
        "\t#     self.M = 1 # Increase the pendency of closed_routes so as to incentivize the solver to pick trains on such routes.\n",
        "\t#     self.df_dist_time = df_dist_time\n",
        "\t#     self.df_pendency = df_pendency\n",
        "\t#     self.df_train_curr_plan = df_train_curr_plan\n",
        "\n",
        "\tWeightage_Load_Pend = 5\n",
        "\tCALCULATE_ALL_PAIRS_SHORTEST = False\n",
        "\tCALCULATE_ALL_BASE_LOC_PAIRS_SHORTEST = True\n",
        "\tINF = 1000000000\n",
        "\tndist = INF # default distance if a route doesn't exists between two points\n",
        "\tntime = INF # default time if a route doesn't exists between two points\n",
        "\tnstack = 2 # Default is considered SS\n",
        "\tnpendency = 0 # Default pendency if path doesn't exist between locations\n",
        "\tcont_dict = {20: 0, 40: 1}\n",
        "\toverall_pendency_data = pendency_data.groupby([\"booking_location_code\",\"to_terminal_code\"])[[\"pendency\",\"empty_pendency\"]].agg('sum').reset_index()\n",
        "\toverall_pendency_data[\"route\"] =  overall_pendency_data[\"booking_location_code\"] + \"-\" + overall_pendency_data[\"to_terminal_code\"]\n",
        "\n",
        "\tdef data_prep(df_dist_time, df_pendency, df_train_curr_plan):\n",
        "\t\t### Distance-Time data\n",
        "\t\t# In stack, SS = 90 and DS = 180 .\n",
        "\t\t# Dist is the rounded off Distance\n",
        "\t\t# Time_min is in minute\n",
        "\t\t# Null values in Time_min is replaced by (distance between this route / average speed of train)\n",
        "\t\tdf_dist_time.rename(columns={'origin_code':'Origin','destination_code':'Destination','rational_distance':'Distance'}, inplace=True)\n",
        "\n",
        "\t\tdf_dist_time['Stack'] = df_dist_time['ss_ds'].apply(lambda x: 90 if x == 'SS' else (180 if x == 'DS' else None))\n",
        "\n",
        "\t\tdf_dist_time['Time_min'] = round(60 * df_dist_time['r_tat_sla'])\n",
        "\t\tdf_dist_time['Dist'] = round(df_dist_time['Distance'])\n",
        "\t\tvalid_rows = df_dist_time['Time_min'].notnull()  # boolean mask for non-null values in 'time'\n",
        "\t\taverage_speed = df_dist_time.loc[valid_rows, 'Dist'].sum() / df_dist_time.loc[valid_rows, 'Time_min'].sum()\n",
        "\t\tdf_dist_time['Time_min'].fillna(df_dist_time['Dist'] / average_speed,inplace=True)\n",
        "\t\t'''# Active_Route\n",
        "\t\tdf_dist_time['Active_Route'] = 1\n",
        "\t\t# Closed Route\n",
        "\t\tdf_dist_time['closed_rt'] = 0\n",
        "\t\tdf_dist_time.loc[(df_dist_time['Origin'] == 'PXXL') & (df_dist_time['Destination'] == 'ZZBM'), 'closed_rt'] = 1\n",
        "\t\tdf_dist_time.loc[(df_dist_time['Origin'] == 'MIIC') & (df_dist_time['Destination'] == 'QQPQ'), 'closed_rt'] = 1\n",
        "\t\t# df_dist_time.loc[(df_dist_time['Origin'] == 'MIIC') & (df_dist_time['Destination'] == 'SSKA'), 'closed_rt'] = 1\n",
        "\t\tdf_dist_time.loc[(df_dist_time['Origin'] == 'ALPK') & (df_dist_time['Destination'] == 'UIV'), 'closed_rt'] = 1\n",
        "\t\t'''\n",
        "\t\tdist_time_df = df_dist_time.copy()\n",
        "\n",
        "\t\t### pendency data\n",
        "\t\tdf_pendency.rename(columns={'pendency':'Pendency','to_terminal_code':'Destination','booking_location_code':'Source'}, inplace=True)\n",
        "\t\t'''\n",
        "\t\tdf_pendency['empty_pendency'] = np.random.uniform(1, 100, size = df_pendency.shape[0])\n",
        "\t\tdf_pendency['empty_pendency'] = round(df_pendency['empty_pendency'])\n",
        "\t\tdf_pendency['cont_size'] = 20\n",
        "\t\tnew_data = {'Pendency': [1500, 15], 'Source': ['KDPO', 'QQPQ'], 'Destination': ['APPS', 'PXXL'], 'odr_days': [6,8], 'empty_pendency':[35,75],'cont_size':[40,40]}\n",
        "\t\t# Creating a new DataFrame with the dummy data for 40 ft containers ##pen41\n",
        "\t\tnew_rows_df = pd.DataFrame(new_data)\n",
        "\t\t# Appending the new rows to the existing DataFrame\n",
        "\t\tdf_pendency = pd.concat([df_pendency, new_rows_df], ignore_index = True)\n",
        "\t\t'''\n",
        "\t\tdf_pendency['Pendency'] = df_pendency['Pendency'].apply(lambda x: max(0, x))\n",
        "\t\tdf_pendency['empty_pendency'] = df_pendency['empty_pendency'].apply(lambda x: max(0, x))\n",
        "\t\tpend_df = df_pendency.copy()\n",
        "\n",
        "\n",
        "\t\t### Train Current Status data\n",
        "\t\t# Rational distance value are replaced by 0 because these trains were at the Terminal. They didn't covered any distance.\n",
        "\t\t# Actual remaining km = Remaining_kms - Distance already covered(rational_distance)\n",
        "\t\tdf_train_curr_plan.rename(columns={'FOIS_ETA':'ETA','rake_name':'Train','origin_code':'Origin','destination_code':'Destination','TXR_Kms_Remaining':'Remaining_kms'},inplace=True)\n",
        "\t\tdf_train_curr_plan['rational_distance'].fillna(0,inplace=True)\n",
        "\t\tdf_train_curr_plan['Actual_remaining_kms'] = round(df_train_curr_plan['Remaining_kms'] - df_train_curr_plan['rational_distance'])\n",
        "\t\tdf_train_curr_plan[\"remaining_min\"] = pd.to_datetime(df_train_curr_plan[\"TXR_Due_Date\"]) - pd.to_datetime(df_train_curr_plan[\"ETA\"])\n",
        "\t\tdf_train_curr_plan[\"remaining_min\"] = round((60*df_train_curr_plan[\"remaining_min\"])//pd.Timedelta(hours=1))\n",
        "\t\t# Replace negative values in 'remaining_min' with 0\n",
        "\t\t# df_train_curr_plan['remaining_min'] = df_train_curr_plan['remaining_min'].apply(lambda x: max(0, x))\n",
        "\t\t# Replace negative values in 'Actual_remaining_kms' with 0\n",
        "\t\t# df_train_curr_plan['Actual_remaining_kms'] = df_train_curr_plan['Actual_remaining_kms'].apply(lambda x: max(0, x))\n",
        "\t\t# df_train_curr_plan.loc[(df_train_curr_plan['Train'] == '11XX'), 'remaining_min'] = 1000\n",
        "\t\ttrain_curr_plan_df = df_train_curr_plan.copy()\n",
        "\n",
        "\t\treturn dist_time_df, pend_df, train_curr_plan_df\n",
        "\n",
        "\n",
        "\tdef get_loc(df, df1, df2):\n",
        "\t\tsrc = sorted(list(df['Origin'].tolist())) # Dist_Time\n",
        "\t\tdest = sorted(list(df['Destination'].tolist())) # Dist_Time\n",
        "\t\tsrc1 = sorted(list(df1['Source'].tolist())) # Pendency\n",
        "\t\tdest1 = sorted(list(df1['Destination'].tolist())) # Pendency\n",
        "\t\tdest2 = sorted(list(df2['Destination'].tolist())) # Train_Curr_Status\n",
        "\t\tbase_depots = sorted(list(df2['base_depot_code'].tolist())) # Train_Curr_Status\n",
        "\t\tlocs = sorted(list(set((src) + (dest) + (src1) + (dest1) + (dest2) + (base_depots)))) # get unique locations\n",
        "\t\tloc_dict = {}\n",
        "\t\tfor idx, loc in enumerate(locs):\n",
        "\t\t\tloc_dict[loc] = idx\n",
        "\t\treturn locs, loc_dict\n",
        "\n",
        "\tdef get_base_depot(df, trains, train_dict):\n",
        "\t\tbase_depot_dict = {}\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t\ttrain, base_depot = df.at[idx, 'Train'], df.at[idx, 'base_depot_code']\n",
        "\t\t\tbase_depot_dict[train] = base_depot\n",
        "\t\tbase_depot_locs = list(df['base_depot_code'].unique().tolist())\n",
        "\t\treturn base_depot_locs, base_depot_dict\n",
        "\n",
        "\tdef get_dist_time_stack(df, num_loc, loc_dict, locs, base_loc, Trains, train_dict, start_loc):\n",
        "\t\tdist = [[ndist] * num_loc for _ in range(num_loc)]\n",
        "\t\ttime = [[ntime] * num_loc for _ in range(num_loc)]\n",
        "\t\tclosed_rt = {}\n",
        "\t\tfor i in range(num_loc):\n",
        "\t\t\tdist[i][i], time[i][i] = 0, 0\n",
        "\t\tstack = [[nstack] * num_loc for _ in range(num_loc)]\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t\tsrc, dest, d, t, s, actve, close_rt = df.at[idx, 'Origin'], df.at[idx, 'Destination'], df.at[idx, 'Dist'], df.at[idx, 'Time_min'], df.at[idx, 'ss_ds'], df.at[idx, 'Active_Route'], df.at[idx, 'closed_rt']\n",
        "\t\t\tval = 2\n",
        "\t\t\tif s == 'DS':\n",
        "\t\t\t\tval = 4\n",
        "\t\t\tdist[loc_dict[src]][loc_dict[dest]] = int(d)\n",
        "\t\t\t# dist[loc_dict[dest]][loc_dict[src]] = d\n",
        "\t\t\ttime[loc_dict[src]][loc_dict[dest]] = int(t)\n",
        "\t\t\t# time[loc_dict[dest]][loc_dict[src]] = t\n",
        "\t\t\tstack[loc_dict[src]][loc_dict[dest]] = val\n",
        "\t\t\tstack[loc_dict[dest]][loc_dict[src]] = val\n",
        "\t\t\tif int(close_rt) == 1:\n",
        "\t\t\t\tclosed_rt[(src, dest)] = 1\n",
        "\n",
        "\t\tclosed_rt_train = {}\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\tk = train_dict[train]\n",
        "\t\t\tclosed_rt_train[k] = 0\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tif (loc1, loc2) in closed_rt:\n",
        "\t\t\t\t\tclosed_rt_train[k] = 1\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\tn = len(dist)\n",
        "\t\tif CALCULATE_ALL_BASE_LOC_PAIRS_SHORTEST:\n",
        "\t\t\t# Calculate all-pairs shortest path\n",
        "\t\t\t# but only consider base_locations\n",
        "\t\t\t# as i -> base or j -> base in (i -> j -> base) can be active or inactive\n",
        "\t\t\tfor k in range(n):\n",
        "\t\t\t\tfor loc1 in locs:\n",
        "\t\t\t\t\tfor bloc2 in base_loc:\n",
        "\t\t\t\t\t\ti, j = loc_dict[loc1], loc_dict[bloc2]\n",
        "\t\t\t\t\t\tif dist[i][k] < INF and dist[k][j] < INF:\n",
        "\t\t\t\t\t\t\tdist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n",
        "\t\t\t\t\t\tif time[i][k] < INF and time[k][j] < INF:\n",
        "\t\t\t\t\t\t\ttime[i][j] = min(time[i][j], time[i][k] + time[k][j])\n",
        "\t\telif CALCULATE_ALL_PAIRS_SHORTEST:\n",
        "\t\t\t# Calculate all-pairs shortest path\n",
        "\t\t\tfor k in range(n):\n",
        "\t\t\t\tfor i in range(n):\n",
        "\t\t\t\t\tfor j in range(n):\n",
        "\t\t\t\t\t\tif dist[i][k] < INF and dist[k][j] < INF:\n",
        "\t\t\t\t\t\t\tdist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n",
        "\t\t\t\t\t\tif time[i][k] < INF and time[k][j] < INF:\n",
        "\t\t\t\t\t\t\ttime[i][j] = min(time[i][j], time[i][k] + time[k][j])\n",
        "\t\treturn dist, time, stack, closed_rt, closed_rt_train\n",
        "\n",
        "\tdef get_trains_rem_dist_time(df):\n",
        "\t\tTrains = df['Train'].unique().tolist()\n",
        "\t\ttrain_dict = {}\n",
        "\t\tfor idx, train in enumerate(Trains):\n",
        "\t\t\ttrain_dict[train] = idx\n",
        "\t\twagons = {}\n",
        "\t\tRem_Dist = {}\n",
        "\t\tRem_Time = {}\n",
        "\t\tstart_loc = {}\n",
        "\t\tEta = {}\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t\ttrain, rem_km, rem_min, starting_loc, wg, eta = df.at[idx, 'Train'], df.at[idx, 'Actual_remaining_kms'], df.at[idx, 'remaining_min'], df.at[idx, 'Destination'], df.at[idx, 'unts'], df.at[idx, 'ETA']\n",
        "\t\t\tRem_Dist[train_dict[train]] = int(rem_km)\n",
        "\t\t\tRem_Time[train_dict[train]] = int(rem_min)\n",
        "\t\t\twagons[train_dict[train]] = int(wg)\n",
        "\t\t\tstart_loc[train] = starting_loc\n",
        "\t\t\tEta[train_dict[train]] = str(eta)\n",
        "\t\treturn Trains, train_dict, Rem_Dist, Rem_Time, start_loc, wagons, Eta\n",
        "\n",
        "\tdef get_pendency(df, num_loc, loc_dict):\n",
        "\t\tpend = [[[npendency, npendency] for _ in range(num_loc)] for _ in range(num_loc)]\n",
        "\t\tempty_pend = [[[npendency, npendency] for _ in range(num_loc)] for _ in range(num_loc)]\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t\tsrc, dest, pendency, e_pend, cont_sz = df.at[idx, 'Source'], df.at[idx, 'Destination'], df.at[idx, 'Pendency'], df.at[idx, 'empty_pendency'], df.at[idx, 'cont_size']\n",
        "\t\t\tx, y= loc_dict[src], loc_dict[dest]\n",
        "\t\t\tpend[x][y][cont_dict[cont_sz]] = int(pendency)\n",
        "\t\t\t# pend[y][x] = pendency\n",
        "\t\t\tempty_pend[x][y][cont_dict[cont_sz]] = int(e_pend)\n",
        "\t\tpend_sums = []\n",
        "\t\tfor i in range(len(pend)):\n",
        "\t\t\tnsum = 0\n",
        "\t\t\tfor j in range(len(pend[i])):\n",
        "\t\t\t\tnsum += pend[i][j][0] + pend[i][j][1]\n",
        "\t\t\tpend_sums.append(nsum)\n",
        "\t\treturn pend, pend_sums, empty_pend\n",
        "\n",
        "\tdef get_pend_updated(pend_df, df_out):\n",
        "\t  if len(df_out) > 0:\n",
        "\t\tdf_pend_cleared_20 = df_out.groupby(['Source', 'Destination'], as_index=False)['Loaded_Pend_Cleared_20'].sum()\n",
        "\t\tdf_empty_cleared_20 = df_out.groupby(['Source', 'Destination'], as_index=False)['Empty_Pend_Cleared_20'].sum()\n",
        "\t\tdf_pend_cleared_40 = df_out.groupby(['Source', 'Destination'], as_index=False)['Loaded_Pend_Cleared_40'].sum()\n",
        "\t\tdf_empty_cleared_40 = df_out.groupby(['Source', 'Destination'], as_index=False)['Empty_Pend_Cleared_40'].sum()\n",
        "\t\tdf_merged = pd.merge(df_pend_cleared_20, pend_df, on=['Source', 'Destination'], how='right')\n",
        "\t\tdf_merged_1 = pd.merge(df_empty_cleared_20, df_merged, on=['Source', 'Destination'], how='right')\n",
        "\t\tdf_merged_40 = pd.merge(df_pend_cleared_40, df_merged_1, on=['Source', 'Destination'], how='right')\n",
        "\t\tdf_merged_1_40 = pd.merge(df_empty_cleared_40, df_merged_40, on=['Source', 'Destination'], how='right')\n",
        "\t\tcols_type_int = ['Loaded_Pend_Cleared_40','Loaded_Pend_Cleared_20','Empty_Pend_Cleared_40','Empty_Pend_Cleared_20']\n",
        "\t\tdf_merged_1_40[cols_type_int] = df_merged_1_40[cols_type_int].replace('', 0)\n",
        "\t\tdf_merged_1_40['pen_units_remaining'] =np.where(df_merged_1_40['cont_size']==40,df_merged_1_40['Pendency'] - df_merged_1_40['Loaded_Pend_Cleared_40'] ,df_merged_1_40['Pendency'] - df_merged_1_40['Loaded_Pend_Cleared_20'])\n",
        "\t\tdf_merged_1_40['pen_units_remaining'].fillna(df_merged_1_40['Pendency'],inplace=True)\n",
        "\t\tdf_merged_1_40['emp_units_remaining'] =np.where(df_merged_1_40['cont_size']==40,df_merged_1_40['empty_pendency'] - df_merged_1_40['Empty_Pend_Cleared_40'] ,df_merged_1_40['empty_pendency'] - df_merged_1_40['Empty_Pend_Cleared_20'])\n",
        "\t\tdf_merged_1_40['emp_units_remaining'].fillna(df_merged_1_40['empty_pendency'],inplace=True)\n",
        "\t\tpend_df_updated = df_merged_1_40[['Source','Destination','pen_units_remaining','emp_units_remaining','odr_days','cont_size']].copy()\n",
        "\t\tpend_df_updated.rename(columns={'pen_units_remaining':'Pendency','emp_units_remaining':'empty_pendency' }, inplace=True)\n",
        "\t\tpend_df_updated.head()\n",
        "\t  else:\n",
        "\t\tpend_df_updated = pend_df\n",
        "\t  return pend_df_updated\n",
        "\n",
        "\tdef optimize(num_trains, num_stations, num_base, num_loc, locs, Trains, train_dict, dist,\n",
        "\t\t\t\t\t\t  time, Rem_Dist, Rem_Time, loc_dict, base_loc, base_depot_dict, pend, empty_pend, stack, start_loc,\n",
        "\t\t\t\t\t\t  pend_sums, wagons, Eta, closed_rt, closed_rt_train, dist_time_df, pend_df, train_curr_plan_df):\n",
        "\n",
        "\t\t# Create the CP-SAT model\n",
        "\t\t# Uses Google CP-SAT Solver\n",
        "\t\tmodel = cp_model.CpModel()\n",
        "\t\t# x_{kij} = 1 if Train k is assigned location pair (i,j) else 0.\n",
        "\t\tx = {}\n",
        "\t\t# y_{kij} -> Loaded_Pendency cleared by train k from i -> j\n",
        "\t\t# t_{kij} -> Empty_Pendency cleared by train k from i -> j\n",
        "\t\t# Upd: 03/01/24 -> Take care of 20ft and 40ft container.\n",
        "\t\t# {y/t}_{kij20} and {y/t}_{kij40}\n",
        "\t\t# {i, j} can be base location as well\n",
        "\t\t# Note: Different num_wagons for trains can be dealt here.\n",
        "\t\t# stack[i][j] will be num_wagons[k] * {SS, DS} for k^th train.\n",
        "\t\ty = {}\n",
        "\t\tt = {}\n",
        "\t\ttrain_start_locs = []\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tk = train_dict[train]\n",
        "\t\t\texpr = []\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\ttrain_start_locs.append(loc1)\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tif loc1 == loc2:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\ti, j = loc_dict[loc1], loc_dict[loc2]\n",
        "\t\t\t\tx[(k, i, j)] = model.NewBoolVar(f\"x_{k}{i}{j}\")\n",
        "\t\t\t\tcapacity = wagons[train_dict[train]] * stack[i][j]\n",
        "\t\t\t\ttotal_loaded_pend = pend[i][j][cont_dict[20]] + pend[i][j][cont_dict[40]] # 20 ft + 40 ft\n",
        "\t\t\t\tlb_20, lb_40 = 0, 0\n",
        "\t\t\t\t# If total_loaded_pendency[i][j] > 0\n",
        "\t\t\t\t# then each scheduled train on [i, j] must carry > 0 units.\n",
        "\t\t\t\t# Note: This behaviour is not applicable if (i->j) is a closed_route.\n",
        "\t\t\t\tif pend[i][j][cont_dict[20]] and (start_loc[train], loc2) not in closed_rt:\n",
        "\t\t\t\t\tlb_20 = 1\n",
        "\t\t\t\tif pend[i][j][cont_dict[40]] and (start_loc[train], loc2) not in closed_rt:\n",
        "\t\t\t\t\tlb_40 = 1\n",
        "\t\t\t\ty[(k, i, j, 20)] = model.NewIntVar(lb_20, min(pend[i][j][cont_dict[20]], capacity), f\"y_{k}{i}{j}{20}\") # 20 ft\n",
        "\t\t\t\ty[(k, i, j, 40)] = model.NewIntVar(lb_40, min(pend[i][j][cont_dict[40]], capacity), f\"y_{k}{i}{j}{40}\") # 40 ft\n",
        "\n",
        "\t\t\t\tfor cont_size in [20, 40]:\n",
        "\t\t\t\t\tyx = model.NewIntVar(0, min(pend[i][j][cont_dict[cont_size]], capacity), f\"yx{k}{i}{j}{cont_size}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(yx, x[(k, i, j)], y[(k, i, j, cont_size)])\n",
        "\t\t\t\t\tb = model.NewBoolVar('b')\n",
        "\t\t\t\t\tmodel.AddModuloEquality(b, yx, 2)\n",
        "\t\t\t\t\tmodel.Add(b == 0)\n",
        "\n",
        "\t\t\t\tlb = 0\n",
        "\t\t\t\tt[(k, i, j, 20)] = model.NewIntVar(lb, min(empty_pend[i][j][cont_dict[20]], capacity), f\"t_{k}{i}{j}{20}\")\n",
        "\t\t\t\tt[(k, i, j, 40)] = model.NewIntVar(lb, min(empty_pend[i][j][cont_dict[40]], capacity), f\"t_{k}{i}{j}{40}\")\n",
        "\t\t\t\t# Make empties_cleared for 20 ft and 40 ft container 'even' if path (i->j) is chosen\n",
        "\t\t\t\tfor cont_size in [20, 40]:\n",
        "\t\t\t\t\ttx = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[cont_size]], capacity), f\"tx{k}{i}{j}{cont_size}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(tx, x[(k, i, j)], t[(k, i, j, cont_size)])\n",
        "\t\t\t\t\tb1 = model.NewBoolVar('b1')\n",
        "\t\t\t\t\tmodel.AddModuloEquality(b1, tx, 2)\n",
        "\t\t\t\t\tmodel.Add(b1 == 0)\n",
        "\t\t\t\texpr.append(x[(k, i, j)])\n",
        "\t\t\t# A train can be assigned to at-most 1 route.\n",
        "\t\t\tmodel.Add(cp_model.LinearExpr.Sum(expr) <= 1)\n",
        "\n",
        "\t\ttrain_start_locs = list(set(train_start_locs))\n",
        "\n",
        "\t\t# If no route (i->j) exists, it won't be considered.\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tk = train_dict[train]\n",
        "\t\t\ti = loc_dict[start_loc[train]]\n",
        "\t\t\tfor loc in locs:\n",
        "\t\t\t\tj = loc_dict[loc]\n",
        "\t\t\t\tif i == j: # or loc == base_depot_dict[train]\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tif dist[i][j] == INF or time[i][j] == INF:\n",
        "\t\t\t\t\tmodel.Add(x[(k, i, j)] == 0)\n",
        "\n",
        "\t\t'''\n",
        "\t\t# Weak Maintenance Constraint\n",
        "\t\t# for closed routes (i -> j)\n",
        "\t\tfor (loc, val) in closed_rt.items():\n",
        "\t\t\tloc1, loc2 = loc\n",
        "\t\t\ti, j = loc_dict[loc1], loc_dict[loc2]\n",
        "\t\t\tfor train in Trains:\n",
        "\t\t\t\tif loc1 != start_loc[train]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tk = train_dict[train]\n",
        "\t\t\t\tmodel.Add(x[(k, i, j)] * dist[i][j] <= Rem_Dist[k])\n",
        "\t\t\t\tmodel.Add(x[(k, i, j)] * time[i][j] <= Rem_Time[k])\n",
        "\t\t'''\n",
        "\n",
        "\t\t# Deal with closed_rt\n",
        "\t\t# For a particular start_loc i, there exists a single j\n",
        "\t\t# such that i->j is a closed route, then the train\n",
        "\t\t# must consider that j\n",
        "\t\tfor (loc, val) in closed_rt.items():\n",
        "\t\t\tloc1, loc2 = loc\n",
        "\t\t\ti, j = loc_dict[loc1], loc_dict[loc2]\n",
        "\t\t\tflag = 0\n",
        "\t\t\tfor train in Trains:\n",
        "\t\t\t\tif loc1 != start_loc[train]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tk = train_dict[train]\n",
        "\t\t\t\tmodel.Add(x[(k, i, j)] == 1)\n",
        "\n",
        "\t\t# Maintenance constraint(strong, makes sure that after the next trip train can\n",
        "\t\t# still return for maintenance without violating constraint):\n",
        "\t\t# This doesn't consider train going directly to base location.\n",
        "\t\t# Distance and Time\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tk = train_dict[train]\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\ti, j, base_depot_idx = loc_dict[loc1], loc_dict[loc2], loc_dict[base_depot_dict[train]]\n",
        "\t\t\t\tif loc1 == loc2 or loc2 == base_depot_dict[train]:\n",
        "\t\t\t\t  continue\n",
        "\t\t\t\t# This takes care of i->j exists but j->base doesn't\n",
        "\t\t\t\tif dist[j][base_depot_idx] == INF or time[j][base_depot_idx] == INF:\n",
        "\t\t\t\t\tmodel.Add(x[(k, i, j)] == 0)\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\t# if (i->j) closed_route then we are\n",
        "\t\t\t\t# not checking strong_maintenance constraint\n",
        "\t\t\t\tif (start_loc[train], loc2) in closed_rt:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tmodel.Add((dist[j][base_depot_idx] * x[(k, i, j)]) <= Rem_Dist[k] - (dist[i][j] * x[(k, i, j)])).OnlyEnforceIf(x[(k, i, j)])\n",
        "\t\t\t\tmodel.Add((time[j][base_depot_idx] * x[(k, i, j)]) <= Rem_Time[k] - (time[i][j] * x[(k, i, j)])).OnlyEnforceIf(x[(k, i, j)])\n",
        "\n",
        "\n",
        "\t\t# Deal with the {dist, time} constraint if train directly goes to base location.\n",
        "\t\t# Not being considered presently.\n",
        "\t\t# Train can always directly go to base location even if it doesn't have\n",
        "\t\t# enough distance/time or even if there isn't any route.\n",
        "\t\t'''\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tk, i, j = train_dict[train], loc_dict[start_loc[train]], loc_dict[base_depot_dict[train]]\n",
        "\t\t\tif i == j:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tmodel.AddLinearConstraint(dist[i][j] * x[(k, i, j)], 0, Rem_Dist[k])\n",
        "\t\t\tmodel.AddLinearConstraint(time[i][j] * x[(k, i, j)], 0, Rem_Time[k])\n",
        "\t\t'''\n",
        "\t\t# For each pair of location (i, j), the sum of {loaded, empty}_pendency (for 20 ft and 40 ft containers)\n",
        "\t\t# cleared across i->j by all chosen trains\n",
        "\t\t# should be less than equal to the amount of {loaded, empty}_pendency from i->j (for 20 ft and 40 ft containers separately).\n",
        "\t\tload_expr = []\n",
        "\t\tempty_expr = []\n",
        "\t\tfor loc1 in train_start_locs:\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tif loc1 == loc2:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\texpr_20 = []\n",
        "\t\t\t\texpr_40 = []\n",
        "\t\t\t\texpr1_20 = []\n",
        "\t\t\t\texpr1_40 = []\n",
        "\t\t\t\tfor train in Trains:\n",
        "\t\t\t\t\tif loc1 != start_loc[train]:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\tk, i, j = train_dict[train], loc_dict[loc1], loc_dict[loc2]\n",
        "\t\t\t\t\tcapacity = wagons[train_dict[train]] * stack[i][j]\n",
        "\n",
        "\t\t\t\t\tyx20 = model.NewIntVar(0, min(pend[i][j][cont_dict[20]], capacity ), f\"yx{k}{i}{j}{20}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(yx20, x[(k, i, j)], y[(k, i, j, 20)])\n",
        "\t\t\t\t\tload_expr.append(yx20)\n",
        "\t\t\t\t\texpr_20.append(yx20)\n",
        "\n",
        "\t\t\t\t\tyx40 = model.NewIntVar(0, min(pend[i][j][cont_dict[40]], capacity ), f\"yx{k}{i}{j}{40}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(yx40, x[(k, i, j)], y[(k, i, j, 40)])\n",
        "\t\t\t\t\tload_expr.append(yx40)\n",
        "\t\t\t\t\texpr_40.append(yx40)\n",
        "\n",
        "\t\t\t\t\ttx20 = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[20]], capacity), f\"tx{k}{i}{j}{20}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(tx20, x[(k, i, j)], t[(k, i, j, 20)])\n",
        "\t\t\t\t\tempty_expr.append(tx20)\n",
        "\t\t\t\t\texpr1_20.append(tx20)\n",
        "\n",
        "\t\t\t\t\ttx40 = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[40]], capacity), f\"tx{k}{i}{j}{40}\")\n",
        "\t\t\t\t\tmodel.AddMultiplicationEquality(tx40, x[(k, i, j)], t[(k, i, j, 40)])\n",
        "\t\t\t\t\tempty_expr.append(tx40)\n",
        "\t\t\t\t\texpr1_40.append(tx40)\n",
        "\n",
        "\t\t\t\tmodel.Add(cp_model.LinearExpr.Sum(expr_20) <= (pend[i][j][cont_dict[20]]))\n",
        "\t\t\t\tmodel.Add(cp_model.LinearExpr.Sum(expr_40) <= (pend[i][j][cont_dict[40]]))\n",
        "\t\t\t\tmodel.Add(cp_model.LinearExpr.Sum(expr1_20) <= (empty_pend[i][j][cont_dict[20]]))\n",
        "\t\t\t\tmodel.Add(cp_model.LinearExpr.Sum(expr1_40) <= (empty_pend[i][j][cont_dict[40]]))\n",
        "\n",
        "\t\t# Deal with Empty_Pendency\n",
        "\t\t# if (loaded_pendency < 50 % of capacity\n",
        "\t\t# then 100 % capacity to be utilized through empty_pendency)\n",
        "\t\t# or (loaded_pendency >= 50 %)\n",
        "\t\t# Note: For double stack, this 50 % becomes 25 %\n",
        "\t\tbt = {}\n",
        "\t\tfor train in Trains:\n",
        "\t\t\tk, i = train_dict[train], loc_dict[start_loc[train]]\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tj = loc_dict[loc2]\n",
        "\t\t\t\tif i == j or start_loc[train] == loc2:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tbt[(k, i, j)] = model.NewBoolVar(f'bt{k}{i}{j}')\n",
        "\t\t\t\tcapacity = wagons[train_dict[train]] * stack[i][j]\n",
        "\n",
        "\t\t\t\tyx20 = model.NewIntVar(0, min(pend[i][j][cont_dict[20]], capacity ), f\"yx{k}{i}{j}{20}\")\n",
        "\t\t\t\tmodel.AddMultiplicationEquality(yx20, x[(k, i, j)], y[(k, i, j, 20)])\n",
        "\t\t\t\tyx40 = model.NewIntVar(0, min(pend[i][j][cont_dict[40]], capacity ), f\"yx{k}{i}{j}{40}\")\n",
        "\t\t\t\tmodel.AddMultiplicationEquality(yx40, x[(k, i, j)], y[(k, i, j, 40)])\n",
        "\t\t\t\tyx = model.NewIntVar(0, min(pend[i][j][cont_dict[20]] + pend[i][j][cont_dict[40]], capacity ), f\"yx{k}{i}{j}\")\n",
        "\t\t\t\tmodel.Add(yx == yx20 + yx40)\n",
        "\n",
        "\t\t\t\t# bt = 1 => loaded_pendency >= 50 % of capacity\n",
        "\t\t\t\t# Note: For double stack, this 50 % becomes 25 %\n",
        "\t\t\t\t# Update 04/01/24: 50 % becomes 100% and 25% becomes 50%\n",
        "\t\t\t\tfactor = 1\n",
        "\t\t\t\tif stack[i][j] == 4:\n",
        "\t\t\t\t\tfactor = 2\n",
        "\t\t\t\tmodel.Add(factor * yx >= capacity).OnlyEnforceIf(bt[(k, i, j)])\n",
        "\t\t\t\tmodel.Add(factor * yx <= (capacity - 1)).OnlyEnforceIf(bt[(k, i, j)].Not())\n",
        "\n",
        "\t\t\t\ttx20 = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[20]], capacity), f\"tx{k}{i}{j}{20}\")\n",
        "\t\t\t\tmodel.AddMultiplicationEquality(tx20, x[(k, i, j)], t[(k, i, j, 20)])\n",
        "\t\t\t\ttx40 = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[40]], capacity), f\"tx{k}{i}{j}{40}\")\n",
        "\t\t\t\tmodel.AddMultiplicationEquality(tx40, x[(k, i, j)], t[(k, i, j, 40)])\n",
        "\t\t\t\ttx = model.NewIntVar(0, min(empty_pend[i][j][cont_dict[20]] + empty_pend[i][j][cont_dict[40]], capacity ), f\"tx{k}{i}{j}\")\n",
        "\t\t\t\tmodel.Add(tx == tx20 + tx40)\n",
        "\n",
        "\t\t\t\t# For DS, sum of loaded_pend + empty_pend for 20 ft containers <= 50% of capacity.\n",
        "\t\t\t\tif stack[i][j] == 4:\n",
        "\t\t\t\t\tmodel.Add(2 * (yx20 + tx20) <= capacity)\n",
        "\n",
        "\t\t\t\t# if (loaded_pendency < 50 % of capacity\n",
        "\t\t\t\t# then 100 % capacity to be utilized through empty_pendency)\n",
        "\t\t\t\t# Note: For double stack, this 50 % becomes 25 %\n",
        "\t\t\t\tmodel.Add(yx + tx <= capacity).OnlyEnforceIf(bt[(k, i, j)]).OnlyEnforceIf(x[(k, i, j)])\n",
        "\t\t\t\t# If (i->j) is a closed_route\n",
        "\t\t\t\t# we ignore < 50 % loaded_pendency constraint on this path\n",
        "\t\t\t\t# Note: For double stack, this 50 % becomes 25 %\n",
        "\t\t\t\tif (start_loc[train], loc2) in closed_rt:\n",
        "\t\t\t\t\tmodel.Add(yx + tx <= capacity).OnlyEnforceIf(bt[(k, i, j)].Not()).OnlyEnforceIf(x[(k, i, j)])\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tmodel.Add(yx + tx == capacity).OnlyEnforceIf(bt[(k, i, j)].Not()).OnlyEnforceIf(x[(k, i, j)])\n",
        "\n",
        "\t\t# Objective changed to max sum over(i, j, k) Weightage_Load_Pend * {y_{kij} * x_{kij}} + {t_{kij} * x{kij}}\n",
        "\t\tmodel.Maximize(Weightage_Load_Pend * cp_model.LinearExpr.Sum(load_expr) + cp_model.LinearExpr.Sum(empty_expr))\n",
        "\n",
        "\t\tprint(\"Model Building Completed\")\n",
        "\t\tsolver = cp_model.CpSolver()\n",
        "\t\tsolver.parameters.log_search_progress = False\n",
        "\t\tsolver.parameters.cp_model_presolve = True\n",
        "\t\tsolver.parameters.max_time_in_seconds = 120\n",
        "\t\tsolver.random_seed = 42\n",
        "\t\tprint(\"Solving Started\")\n",
        "\t\tstatus = solver.Solve(model)\n",
        "\t\tstatus_dict = {\n",
        "\t\t0: \"UNKNOWN: The status of the model is still unknown. A search limit has been reached before any of the statuses below could be determined.\",\n",
        "\t\t1: \"MODEL_INVALID: The given CpModelProto didn't pass the validation step.\",\n",
        "\t\t2: \"FEASIBLE: A feasible solution has been found. But the search was stopped before we could prove optimality.\",\n",
        "\t\t3: \"INFEASIBLE: The problem has been proven infeasible.\",\n",
        "\t\t4: \"OPTIMAL: An optimal feasible solution has been found.\"\n",
        "\t\t}\n",
        "\t\tprint(status_dict[status])\n",
        "\t\tif status == 2 or status == 4:\n",
        "\t\t\tprint(f\"Objective = {solver.ObjectiveValue()}\")\n",
        "\t\t\tcols = ['Train', 'Source', 'Destination', 'closed_rt (i -> j)', 'Loaded_Pend_Cleared_20', 'Pendency_20', 'Loaded_Pend_Cleared_40', 'Pendency_40',\n",
        "\t\t\t\t\t'bt', 'Empty_Pend_Cleared_20', 'Empties_20','Empty_Pend_Cleared_40', 'Empties_40', 'Utilized_Space', 'Capacity', 'Wagons', 'Stack', 'Base_Loc', 'Dist (i -> j)',\n",
        "\t\t\t\t\t'Dist(j -> base)', 'Dist(i -> base)', 'Dist_Travelled', 'Rem_Dist_Km', 'Total_Time_Mins', 'Rem_Time_Mins', 'TXR_Flag']\n",
        "\t\t\tdf_out = pd.DataFrame(columns = cols)\n",
        "\t\t\ttrains_not_scheduled = []\n",
        "\t\t\tstart_loc_not_scheduled = []\n",
        "\t\t\tpend_train_not_scheduled = []\n",
        "\t\t\teta_train_not_scheduled = []\n",
        "\t\t\tclosed_route_train = []\n",
        "\t\t\tfor train in Trains:\n",
        "\t\t\t\tflag = 0\n",
        "\t\t\t\tloc1 = start_loc[train]\n",
        "\t\t\t\tfor loc2 in locs:\n",
        "\t\t\t\t\tif loc1 == loc2:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\tk, i, j = train_dict[train], loc_dict[loc1], loc_dict[loc2]\n",
        "\t\t\t\t\tcapacity = wagons[train_dict[train]] * stack[i][j]\n",
        "\t\t\t\t\tutilized = solver.Value(y[(k, i, j, 20)]) + solver.Value(t[(k, i, j, 20)]) + solver.Value(y[(k, i, j, 40)]) + solver.Value(t[(k, i, j, 40)])\n",
        "\t\t\t\t\tclose_rt_flag = 0\n",
        "\t\t\t\t\tif (loc1, loc2) in closed_rt:\n",
        "\t\t\t\t\t\tclose_rt_flag = 1\n",
        "\t\t\t\t\tif solver.Value(x[(k, i, j)]):\n",
        "\t\t\t\t\t\tflag = 1\n",
        "\t\t\t\t\t\tif loc2 == base_depot_dict[train]:\n",
        "\t\t\t\t\t\t\tbase_idx = loc_dict[base_depot_dict[train]]\n",
        "\t\t\t\t\t\t\ttxr_flag = 0\n",
        "\t\t\t\t\t\t\tif (dist[i][j] + dist[j][base_idx]) > Rem_Dist[k] or (time[i][j] + time[j][base_idx]) > Rem_Time[k]:\n",
        "\t\t\t\t\t\t\t\ttxr_flag = 1\n",
        "\t\t\t\t\t\t\tdata = [train, loc1, loc2, close_rt_flag, solver.Value(y[(k, i, j, 20)]), pend[i][j][cont_dict[20]],\n",
        "\t\t\t\t\t\t\t\t\tsolver.Value(y[(k, i, j, 40)]), pend[i][j][cont_dict[40]], solver.Value(bt[(k, i, j)]), solver.Value(t[(k, i, j, 20)]), empty_pend[i][j][cont_dict[20]],\n",
        "\t\t\t\t\t\t\t\t\tsolver.Value(t[(k, i, j, 40)]), empty_pend[i][j][cont_dict[40]], utilized, capacity, wagons[train_dict[train]],\n",
        "\t\t\t\t\t\t\t\t\tstack[i][j], base_depot_dict[train], 0, 0, dist[i][base_idx], dist[i][base_idx], Rem_Dist[k], time[i][base_idx], Rem_Time[k], txr_flag]\n",
        "\t\t\t\t\t\t\tnew_row = pd.DataFrame([data], columns = cols)\n",
        "\t\t\t\t\t\t\tdf_out = pd.concat([df_out, new_row], ignore_index = True)\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tbase_idx = loc_dict[base_depot_dict[train]]\n",
        "\t\t\t\t\t\t\ttxr_flag = 0\n",
        "\t\t\t\t\t\t\tif (dist[i][j] + dist[j][base_idx]) > Rem_Dist[k] or (time[i][j] + time[j][base_idx]) > Rem_Time[k]:\n",
        "\t\t\t\t\t\t\t\ttxr_flag = 1\n",
        "\t\t\t\t\t\t\tdata = [train, loc1, loc2, close_rt_flag, solver.Value(y[(k, i, j, 20)]), pend[i][j][cont_dict[20]],\n",
        "\t\t\t\t\t\t\t\t\tsolver.Value(y[(k, i, j, 40)]), pend[i][j][cont_dict[40]], solver.Value(bt[(k, i, j)]), solver.Value(t[(k, i, j, 20)]), empty_pend[i][j][cont_dict[20]],\n",
        "\t\t\t\t\t\t\t\t\tsolver.Value(t[(k, i, j, 40)]), empty_pend[i][j][cont_dict[40]], utilized, capacity, wagons[train_dict[train]], stack[i][j],\n",
        "\t\t\t\t\t\t\t\t\tbase_depot_dict[train], dist[i][j], dist[j][base_idx], 0, dist[i][j] + dist[j][base_idx], Rem_Dist[k],\n",
        "\t\t\t\t\t\t\t\t\ttime[i][j] + time[j][base_idx], Rem_Time[k], txr_flag]\n",
        "\t\t\t\t\t\t\tnew_row = pd.DataFrame([data], columns = cols)\n",
        "\t\t\t\t\t\t\tdf_out = pd.concat([df_out, new_row], ignore_index = True)\n",
        "\t\t\t\tif flag == 0:\n",
        "\t\t\t\t\ttrains_not_scheduled.append(train)\n",
        "\t\t\t\t\tstart_loc_not_scheduled.append(start_loc[train])\n",
        "\t\t\t\t\tpend_train_not_scheduled.append((pend_sums[loc_dict[start_loc[train]]]))\n",
        "\t\t\t\t\teta_train_not_scheduled.append(Eta[train_dict[train]])\n",
        "\t\t\t\t\tclosed_route_train.append(closed_rt_train[k])\n",
        "\t\t\tdf_out_1 = pd.DataFrame()\n",
        "\t\t\tdf_out_1['Trains_Not_Scheduled'] = trains_not_scheduled\n",
        "\t\t\tdf_out_1['closed_rt_train'] = closed_route_train\n",
        "\t\t\tdf_out_1['Start_Loc'] = start_loc_not_scheduled\n",
        "\t\t\tdf_out_1['Total Pendency'] = pend_train_not_scheduled\n",
        "\t\t\tdf_out_1['ETA'] = eta_train_not_scheduled\n",
        "\t\t\t# df_out.to_csv('./05122023_output_empty_pend.csv', index = False)\n",
        "\t\t\t# df_out_1.to_csv('./05122023_unscheduled_trains_empty_pend.csv', index = False)\n",
        "\t\t\tprint(f\"Trains_Scheduled = {len(Trains) - len(trains_not_scheduled)}/{len(Trains)}\")\n",
        "\t\t\tpend_df_upd = get_pend_updated(pend_df, df_out)\n",
        "\t\t\t# pend_df_upd = pd.DataFrame()\n",
        "\t\t\treturn df_out, df_out_1, pend_df_upd\n",
        "\n",
        "\tdef nearest_lower_even(num):\n",
        "\t\treturn 2 * (num // 2)\n",
        "\n",
        "\tdef schedule_zero_pend_train(unscheduled_trains, locs, loc_dict, dist, time, stack,\n",
        "\t\t\t\t\t\t\t\t Trains, train_dict, Rem_Dist, Rem_Time, start_loc,\n",
        "\t\t\t\t\t\t\t\t wagons, base_loc, base_depot_dict, updated_pend, pend_sums_upd, empty_pend_upd):\n",
        "\t\tz = {}\n",
        "\t\tfor train in unscheduled_trains:\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\ti, k = loc_dict[loc1], train_dict[train]\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tfor loc3 in locs:\n",
        "\t\t\t\t\tj, m = loc_dict[loc2], loc_dict[loc3]\n",
        "\t\t\t\t\tz[(k, i, j, m)] = 0\n",
        "\n",
        "\t\ttrain_upd_pend_cleared_20 = {}\n",
        "\t\ttrain_upd_pend_cleared_40 = {}\n",
        "\t\ttrain_upd_pend_20 = {}\n",
        "\t\ttrain_upd_pend_40 = {}\n",
        "\t\ttrain_upd_emp_cleared_20 = {}\n",
        "\t\ttrain_upd_emp_cleared_40 = {}\n",
        "\t\ttrain_upd_emp_pend_20 = {}\n",
        "\t\ttrain_upd_emp_pend_40 = {}\n",
        "\t\ttrain_upd_pend_cleared = {}\n",
        "\t\ttrain_upd_emp_cleared = {}\n",
        "\t\ttrain_capacity = {}\n",
        "\t\ttrain_total_pend_cleared = {}\n",
        "\n",
        "\t\tupdated_pend1 = updated_pend.copy()\n",
        "\t\tempty_pend_upd1 =empty_pend_upd.copy()\n",
        "\t\tfor train in unscheduled_trains:\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\ti= loc_dict[loc1]\n",
        "\t\t\tindices = []\n",
        "\t\t\tfor j in range(len(locs)):\n",
        "\t\t\t\tif dist[i][j] == INF:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tindices.append(j)\n",
        "\n",
        "\t\t\tsorted_indices = sorted(indices, key=lambda j: dist[i][j])\n",
        "\t\t\tk = train_dict[train]\n",
        "\t\t\tb = loc_dict[base_depot_dict[train]]\n",
        "\t\t\tfor j in sorted_indices:\n",
        "\t\t\t\tif (Rem_Dist[k] < dist[i][j]) or (Rem_Time[k] < time[i][j]):\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif pend_sums[j] == 0:\n",
        "\t\t\t\t\t\t#sum(pend[j])\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\tindices2 = [m for m in range(len(locs)) if j!= m and dist[j][m] != INF and dist[m][b] != INF]\n",
        "\t\t\t\t\tsorted_indices2 = sorted(indices2, key=lambda m: (2*nearest_lower_even(min((updated_pend[j][m][cont_dict[20]]+updated_pend[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m])) + nearest_lower_even(min((empty_pend_upd[j][m][cont_dict[20]]+empty_pend_upd[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m]))) , reverse=True)\n",
        "\t\t\t\t\tflag = 0\n",
        "\t\t\t\t\tfor m in sorted_indices2:\n",
        "\t\t\t\t\t\tif stack[j][m] == 2:\n",
        "\t\t\t\t\t\t\tif (nearest_lower_even(min((updated_pend[j][m][cont_dict[20]]+updated_pend[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m])) <  1 * wagons[train_dict[train]] * stack[j][m]) and (nearest_lower_even(min((updated_pend[j][m][cont_dict[20]]+updated_pend[j][m][cont_dict[40]]+empty_pend_upd[j][m][cont_dict[20]]+empty_pend_upd[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m])) < wagons[train_dict[train]] * stack[j][m]):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\t\tif (Rem_Dist[k] < dist[i][j] + dist[j][m] + dist[m][b]) or (Rem_Time[k] < time[i][j] + time[j][m] + time[m][b]):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\t\tz[(k, i, j, m)] = 1\n",
        "\t\t\t\t\t\t\ttrain_capacity[k] = wagons[train_dict[train]] * stack[j][m]\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared_40[k] = nearest_lower_even(min(updated_pend[j][m][cont_dict[40]], wagons[train_dict[train]] * stack[j][m]))\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared_20[k] = nearest_lower_even(min(updated_pend[j][m][cont_dict[20]], (wagons[train_dict[train]] * stack[j][m] - train_upd_pend_cleared_40[k])))\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared_40[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] - train_upd_pend_cleared_40[k] , empty_pend_upd[j][m][cont_dict[40]]))\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared_20[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] - train_upd_pend_cleared_40[k] - train_upd_emp_cleared_40[k] , empty_pend_upd[j][m][cont_dict[20]]))\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared[k] = train_upd_pend_cleared_20[k] + train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared[k] = train_upd_emp_cleared_40[k] + train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\t\t\ttrain_total_pend_cleared[k] = train_upd_pend_cleared[k] + train_upd_emp_cleared[k]\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_20[k] = updated_pend[j][m][cont_dict[20]]\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_40[k] = updated_pend[j][m][cont_dict[40]]\n",
        "\t\t\t\t\t\t\tupdated_pend[j][m][cont_dict[20]] = updated_pend[j][m][cont_dict[20]] - train_upd_pend_cleared_20[k]\n",
        "\t\t\t\t\t\t\tupdated_pend[j][m][cont_dict[40]] = updated_pend[j][m][cont_dict[40]] - train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_pend_20[k] = empty_pend_upd[j][m][cont_dict[20]]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_pend_40[k] = empty_pend_upd[j][m][cont_dict[40]]\n",
        "\t\t\t\t\t\t\tempty_pend_upd[j][m][cont_dict[20]] = empty_pend_upd[j][m][cont_dict[20]] - train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\t\t\tempty_pend_upd[j][m][cont_dict[40]] = empty_pend_upd[j][m][cont_dict[40]] - train_upd_emp_cleared_40[k]\n",
        "\t\t\t\t\t\t\tflag = 1\n",
        "\t\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tif (nearest_lower_even(min((updated_pend[j][m][cont_dict[20]]+updated_pend[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m])) <  0.25 * wagons[train_dict[train]] * stack[j][m]) and (nearest_lower_even(min((updated_pend[j][m][cont_dict[20]]+updated_pend[j][m][cont_dict[40]]+empty_pend_upd[j][m][cont_dict[20]]+empty_pend_upd[j][m][cont_dict[40]]), wagons[train_dict[train]] * stack[j][m])) < wagons[train_dict[train]] * stack[j][m]):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\t\tif (Rem_Dist[k] < dist[i][j] + dist[j][m] + dist[m][b] ) or (Rem_Time[k] < time[i][j] + time[j][m] + time[m][b]):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\t\ttrain_capacity[k] = wagons[train_dict[train]] * stack[j][m]\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared_40[k] = nearest_lower_even(min(updated_pend[j][m][cont_dict[40]], wagons[train_dict[train]] * stack[j][m]))\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared_20[k] = nearest_lower_even(min(updated_pend[j][m][cont_dict[20]], (wagons[train_dict[train]] * stack[j][m] - train_upd_pend_cleared_40[k]),wagons[train_dict[train]] * stack[j][m] *0.5))\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared_40[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] - train_upd_pend_cleared_40[k] , empty_pend_upd[j][m][cont_dict[40]]))\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared_20[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] - train_upd_pend_cleared_40[k] - train_upd_emp_cleared_40[k] , empty_pend_upd[j][m][cont_dict[20]],wagons[train_dict[train]] * stack[j][m] *0.5))\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_cleared[k] = train_upd_pend_cleared_20[k] + train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_cleared[k] = train_upd_emp_cleared_40[k] + train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\t\t\ttrain_total_pend_cleared[k] = train_upd_pend_cleared[k] + train_upd_emp_cleared[k]\n",
        "\t\t\t\t\t\t\tif (train_upd_pend_cleared_40[k] + train_upd_pend_cleared_20[k] < 0.5*wagons[train_dict[train]] * stack[j][m]) and (train_total_pend_cleared[k] < wagons[train_dict[train]] * stack[j][m]):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\t\tz[(k, i, j, m)] = 1\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_20[k] = updated_pend[j][m][cont_dict[20]]\n",
        "\t\t\t\t\t\t\ttrain_upd_pend_40[k] = updated_pend[j][m][cont_dict[40]]\n",
        "\t\t\t\t\t\t\tupdated_pend[j][m][cont_dict[20]] = updated_pend[j][m][cont_dict[20]] - train_upd_pend_cleared_20[k]\n",
        "\t\t\t\t\t\t\tupdated_pend[j][m][cont_dict[40]] = updated_pend[j][m][cont_dict[40]] - train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_pend_20[k] = empty_pend_upd[j][m][cont_dict[20]]\n",
        "\t\t\t\t\t\t\ttrain_upd_emp_pend_40[k] = empty_pend_upd[j][m][cont_dict[40]]\n",
        "\t\t\t\t\t\t\tempty_pend_upd[j][m][cont_dict[20]] = empty_pend_upd[j][m][cont_dict[20]] - train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\t\t\tempty_pend_upd[j][m][cont_dict[40]] = empty_pend_upd[j][m][cont_dict[40]] - train_upd_emp_cleared_40[k]\n",
        "\t\t\t\t\t\t\tflag = 1\n",
        "\t\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tif flag:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\n",
        "\t\tcols = ['Train', 'Source(i)', 'j', 'Txr_Flag', 'Loaded_pend_cleared_20(i -> j)','Loaded_pend_cleared_40(i -> j)', 'Emp_pend_cleared_20(i -> j)','Emp_pend_cleared_40(i -> j)','Total_pend_cleared(i -> j)','Capacity:(i->j)', 'Loaded_pend_20(i -> j)','Loaded_pend_40(i -> j)', 'Emp_pend_20(i -> j)','Emp_pend_40(i -> j)', 'm', 'Loaded_pend_cleared_20(j -> m)', 'Loaded_pend_cleared_40(j -> m)', 'Emp_pend_cleared_20(j -> m)','Emp_pend_cleared_40(j -> m)','Total_Loaded_pend_cleared(j -> m)','Total_Emp_pend_cleared(j -> m)','Total_pend_cleared(j -> m)','Capacity:(j->m)', 'Loaded_pend_20(j -> m)','Loaded_pend_40(j -> m)', 'Emp_pend(j -> m)_20','Emp_pend(j -> m)_40']\n",
        "\t\tdf_zero_pend_trains = pd.DataFrame(columns = cols)\n",
        "\t\tfor train in unscheduled_trains:\n",
        "\t\t\tloc1 = start_loc[train]\n",
        "\t\t\ti, k, b = loc_dict[loc1], train_dict[train], loc_dict[base_depot_dict[train]]\n",
        "\t\t\tflag = 0\n",
        "\t\t\tfor loc2 in locs:\n",
        "\t\t\t\tfor loc3 in locs:\n",
        "\t\t\t\t\tj, m = loc_dict[loc2], loc_dict[loc3]\n",
        "\t\t\t\t\tif z[(k, i, j, m)] == 1:\n",
        "\t\t\t\t\t\tdata = [train, loc1, loc2, 0, updated_pend1[i][j][cont_dict[20]], updated_pend1[i][j][cont_dict[40]],  empty_pend_upd1[i][j][cont_dict[20]], empty_pend_upd1[i][j][cont_dict[40]], (updated_pend1[i][j][cont_dict[20]] + updated_pend1[i][j][cont_dict[40]] + empty_pend_upd1[i][j][cont_dict[20]] + empty_pend_upd1[i][j][cont_dict[40]]) ,wagons[train_dict[train]]* stack[i][j], updated_pend1[i][j][cont_dict[20]], updated_pend1[i][j][cont_dict[40]],  empty_pend_upd1[i][j][cont_dict[20]], empty_pend_upd1[i][j][cont_dict[40]],  loc3, train_upd_pend_cleared_20[k],train_upd_pend_cleared_40[k] , train_upd_emp_cleared_20[k], train_upd_emp_cleared_40[k] , train_upd_pend_cleared[k],train_upd_emp_cleared[k] ,train_total_pend_cleared[k], wagons[train_dict[train]] * stack[j][m], train_upd_pend_20[k],train_upd_pend_40[k],  train_upd_emp_pend_20[k], train_upd_emp_pend_40[k]]\n",
        "\t\t\t\t\t\tnew_row =pd.DataFrame([data], columns = cols)\n",
        "\t\t\t\t\t\tdf_zero_pend_trains = pd.concat([df_zero_pend_trains, new_row], ignore_index = True)\n",
        "\t\t\t\t\t\tflag = 1\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\tif flag:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\tif flag == 0:\n",
        "\t\t\t\ttxr_flag = 0\n",
        "\t\t\t\tif dist[i][b] > Rem_Dist[k] or time[i][b] > Rem_Time[k]:\n",
        "\t\t\t\t\ttxr_flag = 1\n",
        "\t\t\t\tpark = f\"{base_depot_dict[train]}\"\n",
        "\t\t\t\tloaded_pend_cleared_ij_20 = ''\n",
        "\t\t\t\tloaded_pend_cleared_ij_40 = ''\n",
        "\t\t\t\tempty_pend_cleared_ij_20 = ''\n",
        "\t\t\t\tempty_pend_cleared_ij_40 = ''\n",
        "\t\t\t\ttotal_pend_cleared_ij = ''\n",
        "\t\t\t\tloaded_pend_ij_20 = ''\n",
        "\t\t\t\tloaded_pend_ij_40 = ''\n",
        "\t\t\t\tempty_pend_ij_20 = ''\n",
        "\t\t\t\tempty_pend_ij_40 = ''\n",
        "\t\t\t\ttrain_cap_ij = ''\n",
        "\t\t\t\tif not txr_flag:\n",
        "\t\t\t\t\tpark = \"(PARK)\"\n",
        "\t\t\t\tif txr_flag and (dist[i][b] == INF or time[i][b] == INF):\n",
        "\t\t\t\t\tpark = \"(PARK_NO_ROUTE_BASE)\"\n",
        "\t\t\t\tif txr_flag == 1 and park != \"(PARK_NO_ROUTE_BASE)\" and i!= b:\n",
        "\t\t\t\t\ttrain_capacity[k] = wagons[train_dict[train]] * stack[i][b]\n",
        "\t\t\t\t\ttrain_cap_ij = train_capacity[k]\n",
        "\t\t\t\t\tif stack[i][b] == 2:\n",
        "\t\t\t\t\t\ttrain_upd_pend_cleared_40[k] = nearest_lower_even(min(updated_pend[i][b][cont_dict[40]], wagons[train_dict[train]] * stack[i][b]))\n",
        "\t\t\t\t\t\ttrain_upd_pend_cleared_20[k] = nearest_lower_even(min(updated_pend[i][b][cont_dict[20]], wagons[train_dict[train]] * stack[i][b] - train_upd_pend_cleared_40[k]))\n",
        "\t\t\t\t\t\ttrain_upd_emp_cleared_40[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] -train_upd_pend_cleared_40[k] , empty_pend_upd[i][b][cont_dict[40]] ))\n",
        "\t\t\t\t\t\ttrain_upd_emp_cleared_20[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] -train_upd_pend_cleared_40[k] -train_upd_emp_cleared_40[k]  , empty_pend_upd[i][b][cont_dict[20]] ))\n",
        "\t\t\t\t\t\ttrain_total_pend_cleared[k] = train_upd_pend_cleared_20[k] +train_upd_pend_cleared_40[k] + train_upd_emp_cleared_20[k] + train_upd_emp_cleared_40[k]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\ttrain_upd_pend_cleared_40[k] = nearest_lower_even(min(updated_pend[i][b][cont_dict[40]], wagons[train_dict[train]] * stack[i][b]))\n",
        "\t\t\t\t\t\ttrain_upd_pend_cleared_20[k] = nearest_lower_even(min(updated_pend[i][b][cont_dict[20]], wagons[train_dict[train]] * stack[i][b] - train_upd_pend_cleared_40[k] , 0.5*wagons[train_dict[train]]*stack[i][b]))\n",
        "\t\t\t\t\t\ttrain_upd_emp_cleared_40[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] -train_upd_pend_cleared_40[k] , empty_pend_upd[i][b][cont_dict[40]] ))\n",
        "\t\t\t\t\t\ttrain_upd_emp_cleared_20[k] = nearest_lower_even(min(train_capacity[k] - train_upd_pend_cleared_20[k] -train_upd_pend_cleared_40[k] -train_upd_emp_cleared_40[k]  , empty_pend_upd[i][b][cont_dict[20]] ,0.5*wagons[train_dict[train]]*stack[i][b] ))\n",
        "\t\t\t\t\t\ttrain_total_pend_cleared[k] = train_upd_pend_cleared_20[k] +train_upd_pend_cleared_40[k] + train_upd_emp_cleared_20[k] + train_upd_emp_cleared_40[k]\n",
        "\t\t\t\t\tloaded_pend_cleared_ij_40 = train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\tloaded_pend_cleared_ij_20 = train_upd_pend_cleared_20[k]\n",
        "\t\t\t\t\tempty_pend_cleared_ij_40 = train_upd_emp_cleared_40[k]\n",
        "\t\t\t\t\tempty_pend_cleared_ij_20 = train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\ttotal_pend_cleared_ij = loaded_pend_cleared_ij_20 +loaded_pend_cleared_ij_40 + empty_pend_cleared_ij_40 +empty_pend_cleared_ij_20\n",
        "\t\t\t\t\ttrain_upd_pend_20[k] = updated_pend[i][b][cont_dict[20]]\n",
        "\t\t\t\t\tloaded_pend_ij_20 = train_upd_pend_20[k]\n",
        "\t\t\t\t\tupdated_pend[i][b][cont_dict[20]] = updated_pend[i][b][cont_dict[20]] - train_upd_pend_cleared_20[k]\n",
        "\t\t\t\t\ttrain_upd_pend_40[k] = updated_pend[i][b][cont_dict[40]]\n",
        "\t\t\t\t\tloaded_pend_ij_40 = train_upd_pend_40[k]\n",
        "\t\t\t\t\tupdated_pend[i][b][cont_dict[40]] = updated_pend[i][b][cont_dict[40]] - train_upd_pend_cleared_40[k]\n",
        "\t\t\t\t\ttrain_upd_emp_pend_20[k] = empty_pend_upd[i][b][cont_dict[20]]\n",
        "\t\t\t\t\tempty_pend_ij_20 = train_upd_emp_pend_20[k]\n",
        "\t\t\t\t\tempty_pend_upd[i][b][cont_dict[20]] = empty_pend_upd[i][b][cont_dict[20]] - train_upd_emp_cleared_20[k]\n",
        "\t\t\t\t\ttrain_upd_emp_pend_40[k] = empty_pend_upd[i][b][cont_dict[40]]\n",
        "\t\t\t\t\tempty_pend_ij_40 = train_upd_emp_pend_40[k]\n",
        "\t\t\t\t\tempty_pend_upd[i][b][cont_dict[40]] = empty_pend_upd[i][b][cont_dict[40]] - train_upd_emp_cleared_40[k]\n",
        "\n",
        "\t\t\t\tdata = [train, loc1, park, txr_flag, loaded_pend_cleared_ij_20, loaded_pend_cleared_ij_40, empty_pend_cleared_ij_20,empty_pend_cleared_ij_40, total_pend_cleared_ij, train_cap_ij,  loaded_pend_ij_20, loaded_pend_ij_40, empty_pend_ij_20, empty_pend_ij_40, '', '', '','','','','','','','','','','']\n",
        "\t\t\t\tnew_row =pd.DataFrame([data], columns = cols)\n",
        "\t\t\t\tdf_zero_pend_trains = pd.concat([df_zero_pend_trains, new_row], ignore_index = True)\n",
        "\t\t\t\tz[(k, i, j, m)] = 1\n",
        "\n",
        "\t\tdf_zero_pend_trains_out = df_zero_pend_trains.rename(columns={'j':'Source','m':'Destination','Loaded_pend_cleared_20(j -> m)':'Loaded_Pend_Cleared_20','Loaded_pend_cleared_40(j -> m)':'Loaded_Pend_Cleared_40','Emp_pend_cleared_20(j -> m)':'Empty_Pend_Cleared_20','Emp_pend_cleared_40(j -> m)':'Empty_Pend_Cleared_40'})\n",
        "\t\tpend_df_upd_intmd = get_pend_updated(pend_df_upd, df_zero_pend_trains_out)\n",
        "\n",
        "\t\t# print(pend_df_upd_intmd.head())\n",
        "\t\tdf_zero_pend_trains_intmd = df_zero_pend_trains.rename(columns={'Source(i)':'Source','j':'Destination','Loaded_pend_cleared_20(j -> m)':'Loaded_Pend_Cleared_20','Loaded_pend_cleared_40(j -> m)':'Loaded_Pend_Cleared_40','Emp_pend_cleared_20(j -> m)':'Empty_Pend_Cleared_20','Emp_pend_cleared_40(j -> m)':'Empty_Pend_Cleared_40'})\n",
        "\t\tpend_df_upd_round2 = get_pend_updated(pend_df_upd_intmd, df_zero_pend_trains_intmd)\n",
        "\n",
        "\t\treturn df_zero_pend_trains, pend_df_upd_round2\n",
        "\n",
        "\tdist_time_df, pend_df, train_curr_plan_df = data_prep(df_dist_time, df_pendency, df_train_curr_plan)\n",
        "\n",
        "\tlocs, loc_dict = get_loc(dist_time_df, pend_df, train_curr_plan_df) # locations are indexed from [0, num_loc - 1]\n",
        "\tTrains, train_dict, Rem_Dist, Rem_Time, start_loc, wagons, Eta = get_trains_rem_dist_time(train_curr_plan_df) # trains are indexed from [0, num_trains - 1]\n",
        "\tbase_loc, base_depot_dict = get_base_depot(train_curr_plan_df, Trains, train_dict)\n",
        "\tdist, time, stack, closed_rt, closed_rt_train = get_dist_time_stack(dist_time_df, len(locs), loc_dict, locs, base_loc, Trains, train_dict, start_loc)\n",
        "\tpend, pend_sums, empty_pend = get_pendency(pend_df, len(locs), loc_dict)\n",
        "\n",
        "\tnum_trains, num_stations, num_base, num_loc = len(Trains), len(locs) - len(base_loc), len(base_loc), len(locs)\n",
        "\tprint(f\"Total_Trains = {len(Trains)}: Total_Stations = {len(locs) - len(base_loc)}: Total_Base_Locs = {len(base_loc)}\")\n",
        "\tprint(\"Model_Building_Started\")\n",
        "\n",
        "\t# Solving the original model\n",
        "\tdf_out, df_out_1, pend_df_upd = optimize(num_trains, num_stations, num_base, num_loc, locs, Trains, train_dict,\n",
        "\t\t\t\tdist, time, Rem_Dist, Rem_Time, loc_dict, base_loc, base_depot_dict, pend, empty_pend, stack, start_loc,\n",
        "\t\t\t\tpend_sums, wagons, Eta, closed_rt, closed_rt_train, dist_time_df, pend_df, train_curr_plan_df)\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "\t# TRANSHIPMENT - HUB PENDENCY\n",
        "\t################################################################################\n",
        "\n",
        "\t# run the first optimizer (done by this step)\n",
        "\t# find out rakes which carry less than full capacity for which hub route is present\n",
        "\t# unschedule those rakes, add back the pendency for those routes to output pendency data of first optimizer\n",
        "\t# find out routes which are common between pendency_data and via_rt data (where route type in via_rt is transshipment),\n",
        "\t# group by and modify those as origin and via instead of origin and destination\n",
        "\n",
        "\t# To handle - Transhipment Hub Pendency\n",
        "\n",
        "\t# rakes where full capacity isn't utlized\n",
        "\n",
        "\tunscheduled_trains_lesscap = []\n",
        "\t# lesscap_trains dictionary includes all the rakes where the capacity is not fully utilized and all the rakes from df_out_1 (unscheduled rakes)\n",
        "\tlesscap_trains = (df_out[df_out.Utilized_Space != df_out.Capacity].set_index('Train')['Source'].to_dict())\n",
        "\n",
        "\tvia_rt_origins = via_rt.origin_code.unique().tolist()\n",
        "\t# unscheduling rakes which are at via-rt origins only\n",
        "\tfor i in lesscap_trains:\n",
        "\t  if lesscap_trains[i] in via_rt_origins:\n",
        "\t\tunscheduled_trains_lesscap.append(i)\n",
        "\n",
        "\t# if any train with unutilized capacity is found then only proceed\n",
        "\tif len(unscheduled_trains_lesscap) > 0:\n",
        "\n",
        "\t  # df_out of all less capacity trains\n",
        "\t  df_out_x = df_out[df_out.Train.isin(unscheduled_trains_lesscap)].reset_index(drop = True)\n",
        "\n",
        "\t  ##############################################################################\n",
        "\t  # PENDENCY_DATA DATA\n",
        "\t  ##############################################################################\n",
        "\n",
        "\t  # add back the pendency for routes found above (where pendency_data is output of pendency_data after first optimizer is run)\n",
        "\t  for i in range(df_out_x.shape[0]):\n",
        "\t\tsource = df_out_x.loc[i,\"Source\"]\n",
        "\t\tdestination = df_out_x.loc[i,\"Destination\"]\n",
        "\t\tloaded_pend_cleared_20 = df_out_x.loc[i,\"Loaded_Pend_Cleared_20\"]\n",
        "\t\tloaded_pend_cleared_40 = df_out_x.loc[i,\"Loaded_Pend_Cleared_40\"]\n",
        "\t\temp_pend_cleared_20 = df_out_x.loc[i,\"Empty_Pend_Cleared_20\"]\n",
        "\t\temp_pend_cleared_40 = df_out_x.loc[i,\"Empty_Pend_Cleared_40\"]\n",
        "\t\t# pend_df_upd is the updated pendency after first optimizer is run\n",
        "\t\tfor j in range(pend_df_upd.shape[0]):\n",
        "\t\t  if ((pend_df_upd.loc[j,\"Source\"] == source) & (pend_df_upd.loc[j,\"Destination\"] == destination)) :\n",
        "\t\t\tif pend_df_upd.loc[j,\"cont_size\"] == 20.0:\n",
        "\t\t\t  pend_df_upd.loc[j,\"Pendency\"] += int(loaded_pend_cleared_20)               # add back loaded pendency\n",
        "\t\t\t  pend_df_upd.loc[j,\"empty_pendency\"] += int(emp_pend_cleared_20)            # add back empty pendency\n",
        "\t\t\tif pend_df_upd.loc[j,\"cont_size\"] == 40.0:\n",
        "\t\t\t  pend_df_upd.loc[j,\"Pendency\"] += int(loaded_pend_cleared_40)               # add back loaded pendency\n",
        "\t\t\t  pend_df_upd.loc[j,\"empty_pendency\"] += int(emp_pend_cleared_40)            # add back empty pendency\n",
        "\n",
        "\t  # unschedule rakes where full capacity isn't utilized are removed from scheduled trains list\n",
        "\t  df_out = df_out[~df_out.Train.isin(unscheduled_trains_lesscap)].reset_index(drop = True)\n",
        "\n",
        "\t  schedule1 = df_out.copy()\n",
        "\n",
        "\t  # delete the previous outputs\n",
        "\t  del df_out\n",
        "\n",
        "\t  # find out routes which are common between pendency_data and via_rt data (where route type in via_rt is Transhipment-Hub),\n",
        "\n",
        "\t  pend_df_upd_x = pend_df_upd.merge(via_rt[via_rt.route_type != \"Maintenance\"][[\"origin_code\",\"destination_code\",\"via\",\"route_type\"]]\n",
        "\t\t\t\t\t\t\t\t\t\t, left_on = [\"Source\",\"Destination\"]\n",
        "\t\t\t\t\t\t\t\t\t\t, right_on = [\"origin_code\",\"destination_code\"]\n",
        "\t\t\t\t\t\t\t\t\t\t, how = 'left')\n",
        "\n",
        "\t  pend_df_upd_x.via = pend_df_upd_x.via.fillna(pend_df_upd_x.Destination)\n",
        "\n",
        "\t  # group by and modify those as origin and via instead of origin and destination for \"Transhipment-Hub\" type via routes\n",
        "\t  for i in range(pend_df_upd_x.shape[0]):\n",
        "\t\tif pend_df_upd_x.loc[i,\"route_type\"] == \"Transhipment-Indirect\":\n",
        "\t\t  continue\n",
        "\t\tpend_df_upd_x.loc[i,\"Destination\"] = pend_df_upd_x.loc[i,\"via\"]\n",
        "\n",
        "\t  pend_df_upd_x = pend_df_upd_x.groupby([\"Source\",\"Destination\",\"cont_size\"])[['Pendency','empty_pendency']].sum().reset_index()\n",
        "\n",
        "\t  new_df_pendency = (df_pendency[[\"Source\",\"Destination\",\"odr_days\",\"cont_size\"]].merge(pend_df_upd_x[[\"Pendency\",\"empty_pendency\",\"Source\",\"Destination\",\"cont_size\"]],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  on = [\"Source\",\"Destination\",\"cont_size\"],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  how = 'inner'))[[\"Pendency\",\"empty_pendency\",\"Source\",\"Destination\",\"odr_days\",\"cont_size\"]]\n",
        "\n",
        "\t  del df_pendency\n",
        "\n",
        "\t  df_pendency = new_df_pendency.reset_index(drop = True)\n",
        "\n",
        "\t  ##############################################################################\n",
        "\t  # DIST TIME DATA\n",
        "\t  ##############################################################################\n",
        "\t  df_dist_time = df_dist_time.reset_index(drop = True)\n",
        "\t  ##############################################################################\n",
        "\t  # TRAIN CURRENT STATUS DATA\n",
        "\t  ##############################################################################\n",
        "\t  lcap_df_train_curr_plan = train_current_status[train_current_status.rake_name.isin(list(df_out_1.Trains_Not_Scheduled.unique()) + unscheduled_trains_lesscap)]\n",
        "\t  # delete previous outputs\n",
        "\t  del df_out_1\n",
        "\t  del df_train_curr_plan\n",
        "\t  df_train_curr_plan = lcap_df_train_curr_plan.reset_index(drop = True)\n",
        "\t  ##############################################################################\n",
        "\t  # OPTIMIZATION\n",
        "\t  ##############################################################################\n",
        "\t  dist_time_df, pend_df, train_curr_plan_df = data_prep(df_dist_time, df_pendency, df_train_curr_plan)\n",
        "\n",
        "\t  locs, loc_dict = get_loc(dist_time_df, pend_df, train_curr_plan_df) # locations are indexed from [0, num_loc - 1]\n",
        "\t  Trains, train_dict, Rem_Dist, Rem_Time, start_loc, wagons, Eta = get_trains_rem_dist_time(train_curr_plan_df) # trains are indexed from [0, num_trains - 1]\n",
        "\t  base_loc, base_depot_dict = get_base_depot(train_curr_plan_df, Trains, train_dict)\n",
        "\t  dist, time, stack, closed_rt, closed_rt_train = get_dist_time_stack(dist_time_df, len(locs), loc_dict, locs, base_loc, Trains, train_dict, start_loc)\n",
        "\t  pend, pend_sums, empty_pend = get_pendency(pend_df, len(locs), loc_dict)\n",
        "\n",
        "\t  num_trains, num_stations, num_base, num_loc = len(Trains), len(locs) - len(base_loc), len(base_loc), len(locs)\n",
        "\t  print(f\"Total_Trains = {len(Trains)}: Total_Stations = {len(locs) - len(base_loc)}: Total_Base_Locs = {len(base_loc)}\")\n",
        "\t  print(\"Model_Building_Started\")\n",
        "\n",
        "\t  # Solving the original model\n",
        "\t  df_out, df_out_1, pend_df_upd = optimize(num_trains, num_stations, num_base, num_loc, locs, Trains, train_dict,\n",
        "\t\t\t  dist, time, Rem_Dist, Rem_Time, loc_dict, base_loc, base_depot_dict, pend, empty_pend, stack, start_loc,\n",
        "\t\t\t  pend_sums, wagons, Eta, closed_rt, closed_rt_train, dist_time_df, pend_df, train_curr_plan_df)\n",
        "\n",
        "\t  schedule1 = pd.concat([schedule1, df_out]).reset_index(drop = True)    # schedule1.append(df_out).reset_index(drop = True)\n",
        "\n",
        "\telse:\n",
        "\n",
        "\t  schedule1 = df_out.copy()\n",
        "\t  df_out_1 = df_out_1.copy()\n",
        "\n",
        "\n",
        "\tdef update_schedules(df):\n",
        "\n",
        "\t  # for unscheduled trains (df_out_1 trains)\n",
        "\t  if 'm' in df.columns:\n",
        "\t\tdf.rename(columns = {'Train':'rake_name', 'Source(i)':'origin_code', 'j':'destination_code',\n",
        "\t\t\t\t\t\t\t 'Txr_Flag':'TXR_Flag',\n",
        "\t\t\t\t\t\t\t 'm':'destination_code2',\n",
        "\t\t\t\t\t\t\t 'Loaded_pend_cleared_20(j -> m)':'loaded_TEUs_evacuated2_20',\n",
        "\t\t\t\t\t\t\t 'Loaded_pend_cleared_40(j -> m)':'loaded_TEUs_evacuated2_40',\n",
        "\t\t\t\t\t\t\t 'Emp_pend_cleared_20(j -> m)':'empty_TEUs_evacuated2_20',\n",
        "\t\t\t\t\t\t\t 'Emp_pend_cleared_40(j -> m)':'empty_TEUs_evacuated2_40',\n",
        "\t\t\t\t\t\t\t 'Total_Loaded_pend_cleared(j -> m)':'Total_loaded_evacuated',\n",
        "\t\t\t\t\t\t\t 'Total_Emp_pend_cleared(j -> m)':'Total_empties_evacuated',\n",
        "\t\t\t\t\t\t\t 'Total_pend_cleared(j -> m)':'Total_TEUs_evacuated'\n",
        "\t\t\t\t\t\t   }, inplace = True)\n",
        "\n",
        "\t\tdf = df.merge(train_curr_plan_df[[\"Train\",\"base_depot_code\",\"unts\"]], left_on = [\"rake_name\"], right_on = [\"Train\"], how = \"left\")\n",
        "\t\tdf = df.merge(overall_pendency_data,\n",
        "\t\t\t\t\t  left_on = [\"destination_code\",\"destination_code2\"],\n",
        "\t\t\t\t\t  right_on = [\"booking_location_code\",\"to_terminal_code\"],\n",
        "\t\t\t\t\t  how = 'left')\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t  # if txr flag is 1 then add 'TXR' at the end\n",
        "\t\t  if df.loc[idx,\"TXR_Flag\"] == 1:\n",
        "\t\t\tdf.loc[idx,\"destination_code\"] = df.loc[idx,\"destination_code\"] + \"-(TXR)\"\n",
        "\t\t  if ((df.loc[idx,\"TXR_Flag\"] == 0) & (df.loc[idx,\"destination_code2\"] != \"\")):\n",
        "\t\t\t  df.loc[idx,\"destination_code\"] = df.loc[idx,\"destination_code\"] + \"-\" + df.loc[idx,\"destination_code2\"]\n",
        "\n",
        "\t\tdf.rename(columns =  {'loaded_TEUs_evacuated2_20':'loaded_TEUs_evacuated_20',\n",
        "\t\t\t\t\t\t\t  'loaded_TEUs_evacuated2_40':'loaded_TEUs_evacuated_40',\n",
        "\t\t\t\t\t\t\t  'empty_TEUs_evacuated2_20' : 'empty_TEUs_evacuated_20',\n",
        "\t\t\t\t\t\t\t  'empty_TEUs_evacuated2_40' : 'empty_TEUs_evacuated_40',\n",
        "\t\t\t\t\t\t\t  'stack2':'stack','unts':'wagons'\n",
        "\t\t\t\t\t\t\t\t\t}, inplace = True)\n",
        "\t\tdf = df[[\"rake_name\",\"origin_code\",\"destination_code\",\"loaded_TEUs_evacuated_20\",\n",
        "\t\t\t\t \"loaded_TEUs_evacuated_40\",\"empty_TEUs_evacuated_20\",\"empty_TEUs_evacuated_40\",\"route\",\"pendency\",\"empty_pendency\"]]\n",
        "\t\tcols_type_int = ['loaded_TEUs_evacuated_20','loaded_TEUs_evacuated_40','empty_TEUs_evacuated_20','empty_TEUs_evacuated_40']\n",
        "\t\tdf[cols_type_int] = df[cols_type_int].replace('', 0)\n",
        "\n",
        "\t  else:\n",
        "\t\t# for scheduled trains (df_out)\n",
        "\t\tdf.rename(columns = {'Train':'rake_name', 'Source':'origin_code', 'Destination':'destination_code',\n",
        "\t\t\t\t\t\t\t'closed_rt (i -> j)':'closed_rt',\n",
        "\t\t\t\t\t\t\t'Loaded_Pend_Cleared_20':'loaded_TEUs_evacuated_20',\n",
        "\t\t\t\t\t\t\t'Pendency_20':'pendency_loaded20',\n",
        "\t\t\t\t\t\t\t'Loaded_Pend_Cleared_40':'loaded_TEUs_evacuated_40',\n",
        "\t\t\t\t\t\t\t'Pendency_40':'pendency_loaded40',\n",
        "\t\t\t\t\t\t\t'Empty_Pend_Cleared_20':'empty_TEUs_evacuated_20',\n",
        "\t\t\t\t\t\t\t'Empties_20':'pendency_empties20',\n",
        "\t\t\t\t\t\t\t'Empty_Pend_Cleared_40':'empty_TEUs_evacuated_40',\n",
        "\t\t\t\t\t\t\t'Empties_40':'pendency_empties40',\n",
        "\t\t\t\t\t\t\t'Wagons':'wagons', 'Stack':'stack', 'Base_Loc':'base_location_code'\n",
        "\t\t\t\t\t\t\t}, inplace = True)\n",
        "\n",
        "\t\tdf = df.merge(overall_pendency_data,\n",
        "\t\t\t\t\t  left_on = [\"origin_code\",\"destination_code\"],\n",
        "\t\t\t\t\t  right_on = [\"booking_location_code\",\"to_terminal_code\"],\n",
        "\t\t\t\t\t  how = 'left')\n",
        "\n",
        "\t\tfor idx in range(df.shape[0]):\n",
        "\t\t  # special case for closed route - for closed route the rake always goes back to predefined destination\n",
        "\t\t  # for such routes, if closed_rt = 1 and txr_flag = 1 then print predefined destination + base location + txr\n",
        "\t\t  if ((df.loc[idx,'closed_rt'] == 1) & (df.loc[idx,'TXR_Flag'] == 1)):\n",
        "\t\t\tdf.loc[idx,\"destination_code\"] = df.loc[idx,\"destination_code\"] + \"-\" + df.loc[idx,\"base_location_code\"] + \"-(TXR)\"\n",
        "\t\t  # if not a closed_rt and txr_flag = 1 then print destination to txr as destination is base in such cases already\n",
        "\t\t  elif ((df.loc[idx,'closed_rt'] == 0) & (df.loc[idx,'TXR_Flag'] == 1)):\n",
        "\t\t\tdf.loc[idx,\"destination_code\"] = df.loc[idx,\"destination_code\"] + \"-(TXR)\"\n",
        "\n",
        "\t\tdf = df[[\"rake_name\",\"origin_code\",\"destination_code\",\n",
        "\t\t\t\t \"loaded_TEUs_evacuated_20\",\n",
        "\t\t\t\t \"loaded_TEUs_evacuated_40\",\n",
        "\t\t\t\t \"empty_TEUs_evacuated_20\",\n",
        "\t\t\t\t \"empty_TEUs_evacuated_40\",\"route\",\"pendency\",\"empty_pendency\"]].reset_index(drop = True)\n",
        "\n",
        "\t  return df\n",
        "\n",
        "\tschedule1 = update_schedules(schedule1)\n",
        "\n",
        "    ################################################################################################################################################################################################################################################\n",
        "\t# Unscheduled rakes from above\n",
        "\t################################################################################################################################################################################################################################################\n",
        "\n",
        "\t# Considering all unscheduled trains for now\n",
        "\tdf_out_1['ETA'] = pd.to_datetime(df_out_1['ETA'])\n",
        "\tdf_out_1=df_out_1.sort_values(by = 'ETA')\n",
        "\tunscheduled_trains = df_out_1['Trains_Not_Scheduled'].tolist()\n",
        "\n",
        "\tupdated_pend, pend_sums_upd, empty_pend_upd = get_pendency(pend_df_upd, len(locs), loc_dict)\n",
        "\n",
        "\tprint(\"Scheduling Unscheduled Trains\")\n",
        "\n",
        "\t# Going train by train\n",
        "\tdf_zero_pend_trains, pend_df_upd_round2 = schedule_zero_pend_train(unscheduled_trains, locs, loc_dict,\n",
        "\t\t\t\t\t\t\t\t\t\t\tdist, time, stack, Trains, train_dict, Rem_Dist,\n",
        "\t\t\t\t\t\t\t\t\t\t\tRem_Time, start_loc, wagons,\n",
        "\t\t\t\t\t\t\t\t\t\t\tbase_loc, base_depot_dict, updated_pend, pend_sums_upd, empty_pend_upd)\n",
        "\n",
        "\tprint(\"Scheduling Unscheduled Trains Completed!\")\n",
        "\n",
        "\t################################################################################\n",
        "\t# Scheduling\n",
        "\t################################################################################\n",
        "\n",
        "\tschedule3 = df_zero_pend_trains.copy().reset_index(drop = True)\n",
        "\tschedule3 = update_schedules(schedule3)\n",
        "\n",
        "\n",
        "    schedule_mail_analysis = pd.concat([schedule1,schedule3]).reset_index(drop = True) #schedule1._append(schedule3)\n",
        "\tschedule_mail_analysis[\"next_trip\"] = schedule_mail_analysis.origin_code + \"-\" + schedule_mail_analysis.destination_code\n",
        "\tcols_type_int = ['loaded_TEUs_evacuated_20','loaded_TEUs_evacuated_40','empty_TEUs_evacuated_20','empty_TEUs_evacuated_40']\n",
        "\tschedule_mail_analysis[cols_type_int] = schedule_mail_analysis[cols_type_int].astype(int)\n",
        "\ttrain_current_status_copy1[\"current_trip\"] = train_current_status_copy1[\"origin_code\"] + \"-\" + train_current_status_copy1[\"destination_code\"]\n",
        "\tschedule_mail_analysis = schedule_mail_analysis.merge(train_current_status_copy1[[\"rake_name\",\"current_trip\",\"FOIS_ETA\",\"rake_status\",\"base_depot_code\"]],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  on = \"rake_name\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  how = \"left\")\n",
        "\tschedule_mail_analysis.rename(columns = {\"FOIS_ETA\":\"currtrp_ETA\"}, inplace = True)\n",
        "\tschedule_mail_analysis = schedule_mail_analysis.sort_values(by = [\"next_trip\",\"currtrp_ETA\",\"pendency\"]).reset_index(drop = True)\n",
        "\n",
        "\tfor i in range(train_current_status.shape[0]):\n",
        "\t  if train_current_status.loc[i,\"origin_code\"] == train_current_status.loc[i,\"destination_code\"]:\n",
        "\t\ttrain_current_status.loc[i,\"TXR_KmsRem_CurrDest\"] = train_current_status.loc[i,\"TXR_Kms_Remaining\"]\n",
        "\tschedule_mail_analysis = schedule_mail_analysis.merge(train_current_status[[\"rake_name\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"TXR_Due_Date\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"TXR_KmsRem_CurrDest\"]],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  left_on = [\"rake_name\"],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  right_on = [\"rake_name\"],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  how = 'left').reset_index(drop = True)\n",
        "\tschedule_mail_analysis[\"loaded_TEUs_evacuated\"] = schedule_mail_analysis[\"loaded_TEUs_evacuated_20\"] + schedule_mail_analysis[\"loaded_TEUs_evacuated_40\"]\n",
        "\tschedule_mail_analysis[\"empty_TEUs_evacuated\"] = schedule_mail_analysis[\"empty_TEUs_evacuated_20\"] + schedule_mail_analysis[\"empty_TEUs_evacuated_40\"]\n",
        "\tschedule_mail_analysis.drop(columns = [\"loaded_TEUs_evacuated_20\",\"loaded_TEUs_evacuated_40\",\n",
        "\t\t\t\t\t\t\t\t\t\t  \"empty_TEUs_evacuated_20\",\"empty_TEUs_evacuated_40\", \"route\"], inplace = True)\n",
        "\tschedule_mail_analysis\n",
        "\n",
        "    ################################################################################################################################################################################################################################################\n",
        "    # Rake Schedule after TXR\n",
        "    ################################################################################################################################################################################################################################################\n",
        "\n",
        "    ################################################################################################################################################################################################################################################\n",
        "\t# Rake Schedule after TXR\n",
        "\t################################################################################################################################################################################################################################################\n",
        "\n",
        "\t# Rake Schedule after TXR\n",
        "\tschedule_mail_analysis_txr = schedule_mail_analysis.copy()\n",
        "\ttxr_trains = []\n",
        "\tfor i in range(schedule_mail_analysis_txr.shape[0]):\n",
        "\t  trip = schedule_mail_analysis_txr.loc[i,\"destination_code\"].split(\"-\")\n",
        "\t  # trip = schedule_mail_analysis_txr.loc[i,\"next_trip\"].split(\"-\")\n",
        "\t  if \"(TXR)\" in trip:\n",
        "\t\ttxr_trains.append(schedule_mail_analysis_txr.loc[i,\"rake_name\"])\n",
        "\n",
        "\n",
        "\tif len(txr_trains) > 0:\n",
        "\t  del df_out, df_out_1, pend_df_upd, df_pendency, df_train_curr_plan\n",
        "\t  ##############################################################################\n",
        "\t  # DATA PREP\n",
        "\t  ##############################################################################\n",
        "\t  # PENDENCY DATA\n",
        "\t  ##############################################################################\n",
        "\t  txr_df_pendency = pendency_data[[\"booking_location_code\",\"to_terminal_code\",\"odr_days\",\"cont_size\"]].merge(pend_df_upd_round2[[\"Pendency\",\"empty_pendency\",\"Source\",\"Destination\",\"cont_size\"]],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   left_on = [\"booking_location_code\",\"to_terminal_code\",\"cont_size\"],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   right_on = [\"Source\",\"Destination\",\"cont_size\"],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   how = 'inner')[[\"Pendency\",\"empty_pendency\",\"Source\",\"Destination\",\"odr_days\",\"cont_size\"]]\n",
        "\t  df_pendency = txr_df_pendency.reset_index(drop = True)\n",
        "\t  ##############################################################################\n",
        "\t  # DIST TIME DATA\n",
        "\t  ##############################################################################\n",
        "\t  df_dist_time = df_dist_time.reset_index(drop = True)\n",
        "\t  ##############################################################################\n",
        "\t  # TRAIN CURRENT STATUS DATA\n",
        "\t  ##############################################################################\n",
        "\t  txr_df_train_curr_plan = train_current_status[train_current_status.rake_name.isin(txr_trains)]\n",
        "\t  txr_df_train_curr_plan.TXR_Due_Date = pd.to_datetime(now) + pd.Timedelta(hours = 12) + pd.Timedelta(hours = 30*12) #12 hours + 30 days added # needs to change later for accuracy of timing\n",
        "\t  txr_df_train_curr_plan.TXR_Due_Date = pd.to_datetime(txr_df_train_curr_plan['TXR_Due_Date']).dt.date\n",
        "\t  txr_df_train_curr_plan.TXR_Kms_Remaining = 9000.0\n",
        "\t  txr_df_train_curr_plan.origin_code = txr_df_train_curr_plan.base_depot_code\n",
        "\t  txr_df_train_curr_plan.destination_code = txr_df_train_curr_plan.base_depot_code\n",
        "\t  txr_df_train_curr_plan.rational_distance = 0.0\n",
        "\t  txr_df_train_curr_plan.TXR_KmsRem_CurrDest = 9000.0\n",
        "\t  df_train_curr_plan = txr_df_train_curr_plan.reset_index(drop = True)\n",
        "\n",
        "\t  ##############################################################################\n",
        "\t  # OPTIMIZATION\n",
        "\t  ##############################################################################\n",
        "\t  dist_time_df, pend_df, train_curr_plan_df = data_prep(df_dist_time, df_pendency, df_train_curr_plan)\n",
        "\n",
        "\t  locs, loc_dict = get_loc(dist_time_df, pend_df, train_curr_plan_df) # locations are indexed from [0, num_loc - 1]\n",
        "\t  Trains, train_dict, Rem_Dist, Rem_Time, start_loc, wagons, Eta = get_trains_rem_dist_time(train_curr_plan_df) # trains are indexed from [0, num_trains - 1]\n",
        "\t  base_loc, base_depot_dict = get_base_depot(train_curr_plan_df, Trains, train_dict)\n",
        "\t  dist, time, stack, closed_rt, closed_rt_train = get_dist_time_stack(dist_time_df, len(locs), loc_dict, locs, base_loc, Trains, train_dict, start_loc)\n",
        "\t  pend, pend_sums, empty_pend = get_pendency(pend_df, len(locs), loc_dict)\n",
        "\n",
        "\t  num_trains, num_stations, num_base, num_loc = len(Trains), len(locs) - len(base_loc), len(base_loc), len(locs)\n",
        "\t  print(f\"Total_Trains = {len(Trains)}: Total_Stations = {len(locs) - len(base_loc)}: Total_Base_Locs = {len(base_loc)}\")\n",
        "\t  print(\"Model_Building_Started\")\n",
        "\n",
        "\t  # Solving the original model\n",
        "\t  df_out, df_out_1, pend_df_upd = optimize(num_trains, num_stations, num_base, num_loc, locs, Trains, train_dict,\n",
        "\t\t\t  dist, time, Rem_Dist, Rem_Time, loc_dict, base_loc, base_depot_dict, pend, empty_pend, stack, start_loc,\n",
        "\t\t\t  pend_sums, wagons, Eta, closed_rt, closed_rt_train, dist_time_df, pend_df, train_curr_plan_df)\n",
        "\n",
        "\t  # # Considering all unscheduled trains for now\n",
        "\t  df_out_1['ETA'] = pd.to_datetime(df_out_1['ETA'])\n",
        "\t  df_out_1=df_out_1.sort_values(by = 'ETA')\n",
        "\t  unscheduled_trains = df_out_1['Trains_Not_Scheduled'].tolist()\n",
        "\n",
        "\t  updated_pend, pend_sums_upd, empty_pend_upd = get_pendency(pend_df_upd, len(locs), loc_dict)\n",
        "\n",
        "\t  print(\"Scheduling Unscheduled Trains\")\n",
        "\n",
        "\t  # # Going train by train\n",
        "\t  df_zero_pend_trains, pend_df_upd_round2 = schedule_zero_pend_train(unscheduled_trains, locs, loc_dict,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  dist, time, stack, Trains, train_dict, Rem_Dist,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  Rem_Time, start_loc, wagons,\n",
        "\t\t\t\t\t\t\t\t\t\t\t  base_loc, base_depot_dict, updated_pend, pend_sums_upd, empty_pend_upd)\n",
        "\n",
        "\t  print(\"Scheduling Unscheduled Trains Completed!\")\n",
        "\n",
        "\t  # ############################################################################\n",
        "\t  # # Scheduling\n",
        "\t  # ############################################################################\n",
        "\n",
        "\t  # after txr schedule of unscheduled trains\n",
        "\t  del schedule3, schedule1\n",
        "\n",
        "\t  schedule1 = df_out.copy().reset_index(drop = True)\n",
        "\t  schedule1 = update_schedules(schedule1)\n",
        "\n",
        "\t  schedule3 = df_zero_pend_trains.copy().reset_index(drop = True)\n",
        "\t  schedule3 = update_schedules(schedule3)\n",
        "\n",
        "\t  schedule_mail_analysis_txr = pd.concat([schedule1,schedule3]).reset_index(drop = True)\n",
        "\n",
        "\t  schedule_mail_analysis_txr = schedule_mail_analysis.merge(schedule_mail_analysis_txr[[\"rake_name\",\"destination_code\"]],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ton = \"rake_name\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thow = \"left\").reset_index(drop = True)\n",
        "\n",
        "\t  # schedule_mail_txr = schedule_mail_analysis_txr.copy().reset_index(drop = True)\n",
        "\n",
        "\t  # Add the destination generated after train's txr is complete\n",
        "\t  for i in range(schedule_mail_analysis_txr.shape[0]):\n",
        "\t\tif pd.isna(schedule_mail_analysis_txr.destination_code_y[i]) == True:\n",
        "\t\t  continue\n",
        "\t\tschedule_mail_analysis_txr.next_trip[i] = schedule_mail_analysis_txr.next_trip[i] + \"-\" + schedule_mail_analysis_txr.destination_code_y[i]\n",
        "\n",
        "\t  schedule_mail_analysis_txr.drop(columns = [\"destination_code_x\",\"destination_code_y\"], inplace = True)\n",
        "\n",
        "\telse:\n",
        "\t  schedule_mail_analysis_txr = schedule_mail_analysis.copy().reset_index(drop = True)\n",
        "\n",
        "\t################################################################################################################################################################################################################################################\n",
        "\t# Via route terminals (No direct route between destination and base location)\n",
        "\t################################################################################################################################################################################################################################################\n",
        "\n",
        "\t# Function to update via-route terminals\n",
        "\tdef update_via_rt_terminals(df_schedule,df_via_rt,row_trip):\n",
        "\t  for i in range(df_schedule.shape[0]):\n",
        "\t\ttrip = df_schedule.loc[i,row_trip]\n",
        "\t\tdf_via_rt1 = df_via_rt[df_via_rt.route_type == \"Maintenance\"].reset_index(drop = True)\n",
        "\t\tfor j in range(df_via_rt1.shape[0]):\n",
        "\t\t  bad_via_trip = df_via_rt1.loc[j,\"route\"]\n",
        "\t\t  good_via_trip = df_via_rt1.loc[j,\"via_route\"]\n",
        "\t\t  if bad_via_trip in trip:\n",
        "\t\t\ttrip = trip.replace(bad_via_trip,good_via_trip)\n",
        "\t\tdf_schedule.loc[i,row_trip] = trip\n",
        "\t  return df_schedule\n",
        "\n",
        "\tschedule_mail = update_via_rt_terminals(schedule_mail_analysis_txr, via_rt, 'next_trip')\n",
        "\n",
        "\t# Function to remove consecutive duplicates of terminals from trip names\n",
        "\tdef remove_duplicate_terminals_from_trips(df, row):\n",
        "\t  for idx in range(df.shape[0]):\n",
        "\t\t# df2 = df.copy()\n",
        "\t\tL = df.loc[idx,row].split('-')\n",
        "\t\tdf.loc[idx,row] = '-'.join([key for key, _group in groupby(L)])\n",
        "\t  return df\n",
        "\n",
        "\tschedule_mail = remove_duplicate_terminals_from_trips(schedule_mail, 'next_trip')\n",
        "\tschedule_mail = remove_duplicate_terminals_from_trips(schedule_mail, 'current_trip')\n",
        "\n",
        "\n",
        "\tschedule_mail\n",
        "\n",
        "    # Once a train is scheduled, the pendency should be updated for that route in the output.\n",
        "\tdef update_pendency_in_ouput(df1, row, row2):\n",
        "\t  all_next_trip = list(df1.next_trip.unique())\n",
        "\t  df = pd.DataFrame()\n",
        "\t  for i in (all_next_trip):\n",
        "\t\tfilt_next_trip = df1[df1.next_trip == i].reset_index(drop = True)\n",
        "\t\tpend_next_trip = filt_next_trip[row][0]\n",
        "\t\tfor i in range(filt_next_trip.shape[0]-1):\n",
        "\t\t  filt_next_trip.loc[i,row] = pend_next_trip\n",
        "\t\t  filt_next_trip.loc[i+1,row] = pend_next_trip - filt_next_trip.loc[i,row2]\n",
        "\t\t  pend_next_trip = filt_next_trip.loc[i+1,row]\n",
        "\t\tdf = df._append(filt_next_trip).reset_index(drop = True)\n",
        "\t  return df\n",
        "\n",
        "\tfinal_df = update_pendency_in_ouput(schedule_mail, 'pendency','loaded_TEUs_evacuated')\n",
        "\tfinal_df = update_pendency_in_ouput(final_df, 'empty_pendency','empty_TEUs_evacuated')\n",
        "\tfinal_df.pendency = final_df.pendency.fillna(0)\n",
        "\tfinal_df.empty_pendency = final_df.empty_pendency.fillna(0)\n",
        "\t# final_df.drop(columns = [\"loaded_TEUs_evacuated_20\",\"loaded_TEUs_evacuated_40\",\"empty_TEUs_evacuated_20\",\"empty_TEUs_evacuated_40\"], inplace = True)\n",
        "\tfinal_df[\"total_pendency\"] = final_df[\"pendency\"] + final_df[\"empty_pendency\"]\n",
        "\tfinal_df[\"TEUs_evacuated\"] = final_df[\"loaded_TEUs_evacuated\"] + final_df[\"empty_TEUs_evacuated\"]\n",
        "\tfinal_df.drop(columns = [\"empty_pendency\",\"pendency\"], inplace = True)\n",
        "\tfinal_df.rename(columns = {\"total_pendency\":\"pendency\"}, inplace = True)\n",
        "\tfinal_df.pendency = final_df.pendency.fillna(0)\n",
        "\tfinal_df = final_df[[\"rake_name\",\"rake_status\",\"currtrp_ETA\",\n",
        "\t\t\t\t\t\t\t\t   \"current_trip\",\"next_trip\",\"pendency\",\n",
        "\t\t\t\t\t\t\t\t   \"TEUs_evacuated\",\"TXR_Due_Date\",\"TXR_KmsRem_CurrDest\",\n",
        "\t\t\t\t\t\t\t\t   \"base_depot_code\"]].reset_index(drop = True)\n",
        "\n",
        "\tfinal_df\n",
        "\n",
        "    data_df = final_df.copy()\n",
        "    del schedule_mail_analysis\n",
        "    schedule_mail_analysis = final_df.copy()\n",
        "    data_df[['visit_port','ob_discharge_port']] = data_df['current_trip'].str.split('-', expand=True)\n",
        "    data_df['planned_destination'] = data_df['next_trip'].str.split('-', expand=True)[1]\n",
        "    data_df['TEUs_evacuated'] = data_df['TEUs_evacuated']\n",
        "    data_df[\"timestamp\"] = now\n",
        "    data_df[\"plan_type\"] = \"\"\n",
        "    data_df_main = data_df[['rake_name','visit_port','ob_discharge_port','planned_destination','TEUs_evacuated','pendency',\n",
        "                          'plan_type','currtrp_ETA','rake_status','timestamp', 'current_trip', 'next_trip', 'base_depot_code', 'TXR_Due_Date',\n",
        "                          'TXR_KmsRem_CurrDest', 'loaded_TEUs_evacuated', 'pendency_loaded', 'empty_TEUs_evacuated', 'pendency_empties']]\n",
        "\n",
        "    data_df_main[['TEUs_evacuated','pendency',\n",
        "                  'TXR_KmsRem_CurrDest',\n",
        "                  'loaded_TEUs_evacuated',\n",
        "                  'pendency_loaded',\n",
        "                  'empty_TEUs_evacuated', 'pendency_empties']] = data_df_main[['TEUs_evacuated','pendency','TXR_KmsRem_CurrDest',\n",
        "                                                                             'loaded_TEUs_evacuated', 'pendency_loaded',\n",
        "                                                                             'empty_TEUs_evacuated', 'pendency_empties']].fillna(-99).astype(int)\n",
        "\n",
        "    data_df_main[['rake_name','visit_port','ob_discharge_port','planned_destination']] = data_df_main[['rake_name','visit_port',\n",
        "                                                                                                     'ob_discharge_port','planned_destination']].astype(str)\n",
        "    data_df_main.rename(columns = {\"timestamp\":\"load_ts\",\"currtrp_ETA\":\"ETA\"},inplace=True)\n",
        "    data_df_main['load_ts'] = pd.to_datetime(data_df_main['load_ts'])\n",
        "    data_df_main['ETA'] = pd.to_datetime(data_df_main['ETA'])\n",
        "    data_df_main['TXR_Due_Date'] = pd.to_datetime(data_df_main['TXR_Due_Date']).dt.date.astype(str)\n",
        "    # 'rake_name', 'rake_status', 'current_trip', 'next_trip', 'TEUs_evacuated', 'pendency', 'currtrp_ETA', 'base_depot_code', 'TXR_Due_Date', 'TXR_KmsRem_CurrDest', 'loaded_TEUs_evacuated', 'pendency_loaded', 'empty_TEUs_evacuated', 'pendency_empties'\n",
        "\n",
        "    print(\"***************LOAD BQ Table******************\")\n",
        "\n",
        "    df_load_bq = data_df_main[['rake_name','visit_port','ob_discharge_port','planned_destination','TEUs_evacuated','pendency','plan_type','ETA','rake_status','load_ts','TXR_KmsRem_CurrDest','loaded_TEUs_evacuated','pendency_loaded','empty_TEUs_evacuated','pendency_empties','current_trip','next_trip','base_depot_code','TXR_Due_Date']]\n",
        "    schema=[{\"name\": \"rake_name\",\"type\": \"STRING\"},{\"name\": \"visit_port\",\"type\": \"STRING\"},{\"name\": \"ob_discharge_port\",\"type\": \"STRING\"},{\"name\": \"planned_destination\",\"type\": \"STRING\"},{\"name\": \"TEUs_evacuated\",\"type\": \"INTEGER\"},{\"name\": \"pendency\",\"type\": \"INTEGER\"},{\"name\": \"plan_type\",\"type\": \"STRING\"},{\"name\": \"eta\",\"type\": \"TIMESTAMP\"},{\"name\": \"rake_status\",\"type\": \"STRING\"},{\"name\": \"load_ts\",\"type\": \"TIMESTAMP\"},{\"name\": \"TXR_KmsRem_CurrDest\",\"type\": \"INTEGER\"},{\"name\": \"loaded_TEUs_evacuated\",\"type\": \"INTEGER\"},{\"name\": \"pendency_loaded\",\"type\": \"INTEGER\"},{\"name\": \"empty_TEUs_evacuated\",\"type\": \"INTEGER\"},{\"name\": \"pendency_empties\",\"type\": \"INTEGER\"},{\"name\": \"current_trip\",\"type\": \"STRING\"},{\"name\": \"next_trip\",\"type\": \"STRING\"},{\"name\": \"base_depot_code\",\"type\": \"STRING\"},{\"name\": \"TXR_Due_Date\",\"type\": \"STRING\"}]\n",
        "\n",
        "    df_load_bq.columns=df_load_bq.columns.str.lower()\n",
        "    df_load_bq.to_gbq('apsez-svc-prod-datalake.logistics_cleansed.layer2_route_optimization_plan',project_id='apsez-svc-prod-datalake',table_schema=schema,if_exists='append')\n",
        "    print(\"loaded {0} rows to {1} \".format(df_load_bq.shape[0],'logistics_cleansed.layer2_route_optimization_plan'))\n",
        "\n",
        "    print(\"************ Email Notification *********************\")\n",
        "\n",
        "    schedule_mail_analysis.rename(columns={\"currtrp_ETA\":\"Current_Trip_ETA\"})\n",
        "    schedule_mail_analysis.columns = schedule_mail_analysis.columns.str.upper().str.replace(\"_\",\" \")\n",
        "\n",
        "    def trigger_dag_gcf(data, context=None):\n",
        "\n",
        "      print(f\"event data - {data}\")\n",
        "      web_server_url = (\n",
        "          \"https://2213b29f81334c26ad603a071a655969-dot-asia-south1.composer.googleusercontent.com\"\n",
        "      )\n",
        "      # Replace with the ID of the DAG that you want to run.\n",
        "      dag_id = 'email_alert_rail_route_optimizer'\n",
        "\n",
        "      print(composer2_airflow_rest_api.trigger_dag(web_server_url, dag_id, data))\n",
        "\n",
        "    #Email_alert([],[],[],schedule_mail_analysis)\n",
        "\n",
        "    trigger_dag_gcf(cloud_event.data)\n",
        "\n",
        "    print(schedule_mail_analysis)\n",
        "    return \"done\""
      ]
    }
  ]
}